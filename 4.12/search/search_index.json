{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p> Data quality profiling and exploratory data analysis are crucial steps in the process of Data Science and Machine Learning development.  YData-profiling is a leading tool in the data understanding step of the data science workflow as a pioneering Python package. </p> <p><code>ydata-profiling</code> is a leading package for data profiling, that automates and standardizes the generation of detailed reports,  complete with statistics and visualizations. The significance of the package lies in how it streamlines the process of understanding and preparing data for analysis in a single line of code! If you're ready to get started see the quickstart!</p> <p>Profiling and scale and for databases</p> <p>Take your data profiling to the next level - try ydata-profiling at scale and for databases! </p> <p>Experience enterprise-level scalability and database support while enjoying the familiar open-source features you love.  Dive into large datasets with ease and ensure data quality like never before. Try YData Fabric community version! </p> <p></p>"},{"location":"#why-use-ydata-profiling","title":"Why use ydata-profiling?","text":"<p><code>ydata-profiling</code> is a valuable tool for data scientists and analysts because it streamlines EDA, provides comprehensive insights, enhances data quality, and promotes data science best practices.</p> <ul> <li>Simple to user: It is so simple to use - a single line of code is what you need to get you started. Do you really need more to convince you? \ud83d\ude1b <pre><code>import pandas as pd\nfrom ydata_profiling import ProfileReport\n\ndf = pd.read_csv('data.csv')\nprofile = ProfileReport(df, title=\"Profiling Report\")\n</code></pre></li> <li>Comprehensive insights in a report: a report including a wide range of statistics and visualizations,  providing a holistic view of your data. The report is shareable as a html file or while integrate as a widget in a Jupyter Notebook. </li> <li>Data quality assessment: excel at the identification of missing data, duplicate entries and outliers. These insights are essential for data cleaning and preparation, ensuring the reliability of your analysis and leading to early problems' identification.</li> <li>Ease of integration with other flows: all metrics of the data profiling can be consumed in a standard JSON format.</li> <li>Data exploration for large datasets: even with dataset with a large number of rows, <code>ydata-profiling</code> will be able to help you as it supports both Pandas Dataframes and Spark Dataframes.</li> </ul> <p>To learn more about the package check out concepts overview.</p>"},{"location":"#features-functionalities-integrations","title":"\ud83d\udcdd Features, functionalities &amp; integrations","text":"<p>YData-profiling can be used to deliver a variety of different applications. The documentation includes guides, tips and tricks for tackling them:</p> <p>Data Catalog with data profiling for databases &amp; storages</p> <p>Need to profile directly from databases and data storages (Oracle, snowflake, PostGreSQL, GCS, S3, etc.)?</p> <p>Try YData Fabric Data Catalog for interactive and scalable data profiling</p> <p>Check out the free Community Version.</p> Features &amp; functionalities Description Comparing datasets Comparing multiple version of the same dataset Profiling a Time-Series dataset Generating a report for a time-series dataset with a single line of code Profiling large datasets Tips on how to prepare data and configure <code>ydata-profiling</code> for working with large datasets Handling sensitive data Generating reports which are mindful about sensitive data in the input dataset Dataset metadata and data dictionaries Complementing the report with dataset details and column-specific data dictionaries Customizing the report's appearance Changing the appearance of the report's page and of the contained visualizations Profiling Relational databases ** For a seamless profiling experience in your organization's databases, check Fabric Data Catalog, which allows to consume data from different types of storages such as RDBMs (Azure SQL, PostGreSQL, Oracle, etc.) and object storages (Google Cloud Storage, AWS S3, Snowflake, etc.), among others. PII classification &amp; management ** Automated PII classification and management through an UI experience"},{"location":"#tutorials","title":"Tutorials","text":"<p>Looking for how to use certain features or how to integrate <code>ydata-profiling</code> in your currect stack and workflows, check our step-by-step tutorials. </p> <ul> <li>How to master exploratory data analysis with ydata-profiling? Check this step-by-step tutorial.</li> <li>Looking on how to do exploratory data analysis for Time-series \ud83d\udd5b? Check how to in this  blogpost. To learn more about this feature check the documentation.</li> <li>How to compare 2 datasets? We got you covered with this step-by-step tutorial To learn more about this feature check the documentation.</li> <li>Want to scale for larger datasets? Check the information about release with \u2b50\u26a1Spark support! For more information about spark integration check the documentation</li> </ul>"},{"location":"#support","title":"\ud83d\ude4b Support","text":"<p>Need help? Want to share a perspective? Report a bug? Ideas for collaborations? Reach out via the following channels:</p> <ul> <li>Stack Overflow: ideal for asking questions on how to use the package</li> <li>GitHub Issues: bugs, proposals for changes, feature requests</li> <li>Discord: ideal for projects discussions, ask questions, collaborations, general chat</li> </ul> <p>Help us prioritizing - before reporting, double check, it is always better to upvote!</p> <p>Before reporting an issue on GitHub, check out Common Issues.</p> <p>If you want to validate if your request was prioritized check the project pipeline details</p>"},{"location":"#contributing","title":"\ud83e\udd1d\ud83c\udffd Contributing","text":"<p>Learn how to get involved in the Contribution Guide.</p> <p>A low-threshold place to ask questions or start contributing is the Data Centric AI Community's Discord.</p> <p>A big thank you to all our amazing contributors! </p>"},{"location":"#we-need-your-help-spark","title":"\u26a1 We need your help - Spark!","text":"<p>Spark support has been released, but we are always looking for an extra pair of hands \ud83d\udc50. Check current work in progress!.</p>"},{"location":"advanced_settings/analytics/","title":"Analytics &amp; Telemetry","text":""},{"location":"advanced_settings/analytics/#overview","title":"Overview","text":"<p><code>ydata-profiling</code> is a powerful library designed to generate profile reports from pandas and Spark Dataframe objects.  As part of our ongoing efforts to improve user experience and functionality, <code>ydata-profiling</code>  includes a telemetry feature. This feature collects anonymous usage data, helping us understand how the library is used and identify areas for improvement.</p> <p>The primary goal of collecting telemetry data is to:</p> <ul> <li>Enhance the functionality and performance of the ydata-profiling library</li> <li>Prioritize new features based on user engagement</li> <li>Identify common issues and bugs to improve overall user experience</li> </ul>"},{"location":"advanced_settings/analytics/#data-collected","title":"Data Collected","text":"<p>The telemetry system collects non-personal, anonymous information such as:</p> <ul> <li>Python version</li> <li><code>ydata-profiling</code> version</li> <li>Frequency of use of <code>ydata-profiling</code> features</li> <li>Errors or exceptions thrown within the library</li> </ul>"},{"location":"advanced_settings/analytics/#disabling-usage-analytics","title":"Disabling usage analytics","text":"<p>We respect your choice to not participate in our telemetry collection. If you prefer to disable telemetry, you can do so by setting an environment variable on your system. Disabling telemetry will not affect the functionality  of the ydata-profiling library, except for the ability to contribute to its usage analytics.</p>"},{"location":"advanced_settings/analytics/#set-an-environment-variable","title":"Set an Environment Variable","text":"<p>Open your terminal or command prompt and set the YDATA_PROFILING_NO_ANALYTICS environment variable to false.</p> <pre><code>    import os \n\n    os.environ['YDATA_PROFILING_NO_ANALYTICS'] = 'True'\n</code></pre> <p></p>"},{"location":"advanced_settings/available_settings/","title":"Available Settings","text":"<p>A set of options is available in order to customize the behaviour of <code>ydata-profiling</code> and the appearance of the generated report. The depth of customization allows the creation of behaviours highly targeted at the specific dataset being analysed. The available settings are listed below. To learn how to change them, check :doc:<code>changing_settings</code>.</p>"},{"location":"advanced_settings/available_settings/#general-settings","title":"General settings","text":"<p>Global report settings: </p> Parameter Type Default Description <code>title</code> string Pandas Profiling Report Title for the report, shown in the header and title bar. <code>pool_size</code> integer 0 Number of workers in thread pool. When set to zero, it is set to the number of CPUs available. <code>progress_bar</code> boolean <code>True</code> If <code>True</code>, <code>ydata-profiling</code> will display a progress bar."},{"location":"advanced_settings/available_settings/#variable-summary-settings","title":"Variable summary settings","text":"<p>Settings related with the information displayed for each variable. </p> Parameter Type Default Description <code>sort</code> None, asc or desc nan Sort the variables asc (ending), desc (ending) or None (leaves original sorting). <code>variables.descriptions</code> dict {} Ability to display a description alongside the descriptive statistics of each variable ({'var_name': 'Description'}). <code>vars.num.quantiles</code> list[float] [0.05,0.25,0.5,0.75,0.95] The quantiles to calculate. Note that .25, .5 and .75 are required for the computation of other metrics (median and IQR). <code>vars.num.skewness_threshold</code> integer 20 Warn if the skewness is above this threshold. <code>vars.num.low_categorical_threshold</code> integer 5 If the number of distinct values is smaller than this number, then the series is considered to be categorical. Set to 0 to disable. <code>vars.num.chi_squared_threshold</code> float 0.999 Set to 0 to disable chi-squared calculation. <code>vars.cat.length</code> boolean <code>True</code> Check the string length and aggregate values (min, max, mean, media). <code>vars.cat.characters</code> boolean <code>False</code> Check the distribution of characters and their Unicode properties. Often informative, but may be computationally expensive. <code>vars.cat.words</code> boolean <code>False</code> Check the distribution of words. Often informative, but may be computationally expensive. <code>vars.cat.cardinality_threshold</code> integer 50 Warn if the number of distinct values is above this threshold. <code>vars.cat.imbalance_threshold</code> float 0.5 Warn if the imbalance score is above this threshold. <code>vars.cat.n_obs</code> integer 5 Display this number of observations. <code>vars.cat.chi_squared_threshold</code> float 0.999 Same as above, but for categorical variables. <code>vars.bool.n_obs</code> integer 3 Same as above, but for boolean variables. <code>vars.bool.imbalance_threshold</code> float 0.5 Warn if the imbalance score is above this threshold. Configuration example<pre><code>  profile = df.profile_report(\n      sort=\"ascending\",\n      vars={\n          \"num\": {\"low_categorical_threshold\": 0},\n          \"cat\": {\n              \"length\": True,\n              \"characters\": False,\n              \"words\": False,\n              \"n_obs\": 5,\n          },\n      },\n  )\n\n  profile.config.variables.descriptions = {\n      \"files\": \"Files in the filesystem\",\n      \"datec\": \"Creation date\",\n      \"datem\": \"Modification date\",\n  }\n\n  profile.to_file(\"report.html\")\n</code></pre>"},{"location":"advanced_settings/available_settings/#setting-dataset-schema-type","title":"Setting dataset schema type","text":"<p>Configure the schema type for a given dataset.</p> Set the variable type schema to Generate the profile report<pre><code>  import json\n  import pandas as pd\n\n  from ydata_profiling import ProfileReport\n  from ydata_profiling.utils.cache import cache_file\n\n  file_name = cache_file(\n      \"titanic.csv\",\n      \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\",\n  )\n  df = pd.read_csv(file_name)\n\n  type_schema = {\"Survived\": \"categorical\", \"Embarked\": \"categorical\"}\n\n  # We can set the type_schema only for the variables that we are certain of their types.\n  # All the other will be automatically inferred.\n  report = ProfileReport(df, title=\"Titanic EDA\", type_schema=type_schema)\n\n  report.to_file(\"report.html\")\n</code></pre>"},{"location":"advanced_settings/available_settings/#missing-data-overview-plots","title":"Missing data overview plots","text":"<p>Settings related with the missing data section and the visualizations it can include. </p> Parameter Type Default Description <code>missing_diagrams.bar</code> boolean <code>True</code> Display a bar chart with counts of missing values for each column. <code>missing_diagrams.matrix</code> boolean <code>True</code> Display a matrix of missing values. Similar to the bar chart, but might provide overview of the co-occurrence of missing values in rows. <code>missing_diagrams.heatmap</code> boolean <code>True</code> Display a heatmap of missing values, that measures nullity correlation (i.e. how strongly the presence or absence of one variable affects the presence of another). Configuration example: disable heatmap for large datasets<pre><code>  profile = df.profile_report(\n      missing_diagrams={\n          \"heatmap\": False,\n      }\n  )\n  profile.to_file(\"report.html\")\n</code></pre>"},{"location":"advanced_settings/available_settings/#correlations","title":"Correlations","text":"<p>Settings regarding correlation metrics and thresholds.   The default value is <code>auto</code>, but the following correlation matrices are available:</p> Parameter Description <code>auto</code> Calculates the column pairwise correlation depending on the type schema: - numerical to numerical variable: Spearman correlation coefficient - categorical to categorical variable: Cramer's V association coefficient - numerical to categorical: Cramer's V association coefficient with the numerical variable discretized automatically <code>spearman</code> Spearman's correlation measures the strength and direction of monotonic association between two variables. Great to evaluate the strength of the relation between categorical or ordinal variables. <code>pearson</code> The Pearson correlation coefficient is the most common way of measuring a linear correlation. It is a number between \u20131 and 1 that measures the strength and direction of the relationship between two variables. <code>kendall</code> Kendall rank correlation coefficient is a statistic used to measure the ordinal association between two measured quantities. Kendall's is often used when data doesn't meet one of the requirements of Pearson's correlation. <code>phi_k</code> Phi K is especially suitable for working with mixed-type variables. Using this coefficient we can find (un)expected correlation and evaluate their statistical significance. <code>cramers</code> Cramers is a correlation matrix that is commonly used to examine the association between categorical variables when there is more than 2x2 contingency. <p>For each correlation matrix you can use the following configurations:</p> Parameter Type Default Description <code>correlations.auto.calculate</code> boolean <code>True</code> Whether to compute 'auto' correlation <code>correlations.auto.warn_high_correlations</code> boolean <code>True</code> Show warning for correlations higher than the threshold <code>correlations.auto.threshold</code> float 0.9 Warning threshold <code>correlations.pearson.calculate</code> boolean <code>False</code> Whether to calculate Pearson correlation <code>correlations.pearson.warn_high_correlations</code> boolean <code>True</code> Show warning for correlations higher than the threshold <code>correlations.pearson.threshold</code> float 0.9 Warning threshold <code>correlations.spearman.calculate</code> boolean <code>False</code> Whether to calculate Spearman correlation <code>correlations.spearman.warn_high_correlations</code> boolean <code>False</code> Show warning for correlations higher than the threshold <code>correlations.spearman.threshold</code> float 0.9 Warning threshold <code>correlations.kendall.calculate</code> boolean <code>False</code> Whether to calculate Kendall rank correlation <code>correlations.kendall.warn_high_correlations</code> boolean <code>False</code> Show warning for correlations higher than the threshold <code>correlations.kendall.threshold</code> float 0.9 Warning threshold <code>correlations.phi_k.calculate</code> boolean <code>False</code> Whether to calculate Phi K correlation <code>correlations.phi_k.warn_high_correlations</code> boolean <code>False</code> Show warning for correlations higher than the threshold <code>correlations.phi_k.threshold</code> float 0.9 Warning threshold <code>correlations.cramers.calculate</code> boolean <code>False</code> Whether to calculate Cramer's V association coefficient <code>correlations.cramers.warn_high_correlations</code> boolean <code>True</code> Show warning for correlations higher than the threshold <code>correlations.cramers.threshold</code> float 0.9 Warning threshold <p>For instance, to disable all correlation computations (might be relevant for large datasets):</p> Disabling all correlation matrices<pre><code>    profile = df.profile_report(\n        title=\"Report without correlations\",\n        correlations={\n            \"auto\": {\"calculate\": False},\n            \"pearson\": {\"calculate\": False},\n            \"spearman\": {\"calculate\": False},\n            \"kendall\": {\"calculate\": False},\n            \"phi_k\": {\"calculate\": False},\n            \"cramers\": {\"calculate\": False},\n        },\n    )\n\n    # or using a shorthand that is available for correlations\n    profile = df.profile_report(\n        title=\"Report without correlations\",\n        correlations=None,\n    )\n</code></pre>"},{"location":"advanced_settings/available_settings/#interactions","title":"Interactions","text":"<p>Settings related with the interactions section.  </p> Parameter Type Default Description <code>interactions.continuous</code> boolean <code>True</code> Generate a 2D scatter plot (or hexagonal binned plot) for all continuous variable pairs. <code>interactions.targets</code> list [] When a list of variable names is given, only interactions between these and all other variables are computed."},{"location":"advanced_settings/available_settings/#reports-appearance","title":"Report's appearance","text":"<p>Settings related with the appearance and style of the report.</p> Parameter Type Default Description <code>html.minify_html</code> bool <code>True</code> If <code>True</code>, the output HTML is minified using the <code>htmlmin</code> package. <code>html.use_local_assets</code> bool <code>True</code> If <code>True</code>, all assets (stylesheets, scripts, images) are stored locally. If <code>False</code>, a CDN is used for some stylesheets and scripts. <code>html.inline</code> boolean <code>True</code> If <code>True</code>, all assets are contained in the report. If <code>False</code>, then a web export is created, where all assets are stored in the '[REPORT_NAME]_assets/' directory. <code>html.navbar_show</code> boolean <code>True</code> Whether to include a navigation bar in the report <code>html.style.theme</code> string <code>None</code> Select a bootswatch theme. Available options: flatly (dark blue) and united (orange) <code>html.style.logo</code> string nan A base64 encoded logo, to display in the navigation bar. <code>html.style.primary_color</code> string #337ab7 The primary color to use in the report. <code>html.style.full_width</code> boolean <code>False</code> By default, the width of the report is fixed. If set to <code>True</code>, the full width of the screen is used. <p></p>"},{"location":"advanced_settings/caching/","title":"Caching results throughout multiple runs","text":"<p>The <code>ProfileReport</code> object caches intermediary results for improved performance, which exported HTML and JSON files will reuse. </p> <p>If you modify the configuration in-between runs, either a new <code>ProfileReport</code> object should be created or relevant cached values should be invalidated through <code>report.invalidate_cache()</code>. The specific set of caches to reset can be passed as an argument to the <code>\u00ecnvalidate_cache()</code> method: </p> <ul> <li>rendering to invalidate previously rendered reports (HTML, JSON or widgets)</li> <li>report to remove the caching of the report's structure</li> <li>None (default) to invalidate all caches</li> </ul> <p></p>"},{"location":"advanced_settings/changing_settings/","title":"Changing settings","text":"<p>There are three ways to change the settings listed in <code>available_settings</code>:</p> <ul> <li>through code</li> <li>through a custom configuration file</li> <li>through environment variables</li> </ul>"},{"location":"advanced_settings/changing_settings/#through-code","title":"Through code","text":"Configuration example<pre><code># Create the ProfileReport object without specifying a DataFrame\nprofile = df.profile_report(title=\"Profiling Report\", pool_size=1)\n\n# Change the configuration as desired\nprofile.config.html.minify_html = False\n\n# Specify a DataFrame and trigger the report's computation\nprofile.to_file(\"output.html\")\n</code></pre> <p>Some related settings are grouped together in configuration shorthands, making it easy to selectively enable or disable certain report sections or functionality:</p> <ul> <li><code>samples</code>: control whether the dataset preview is shown.</li> <li><code>correlation</code>: control whether correlation computations are     executed.</li> <li><code>missing_diagrams</code>: control whether missing value analysis is     executed.</li> <li><code>duplicates</code>: control whether duplicate rows are previewed.</li> <li><code>interactions</code>: control whether interactions are computed.</li> </ul> Disable samples, correlations, missing diagrams and duplicates at once<pre><code>r = ProfileReport(\n    samples=None,\n    correlations=None,\n    missing_diagrams=None,\n    duplicates=None,\n    interactions=None,\n)\n</code></pre>"},{"location":"advanced_settings/changing_settings/#through-a-custom-configuration-file","title":"Through a custom configuration file","text":"<p>To control <code>ydata-profiling</code> through a custom file, you can start with one of the sample configuration files below:</p> <ul> <li>default configuration     file     (default)</li> <li>minimal configuration     file     (minimal computation, optimized for performance)</li> </ul> <p>Change the configuration to your liking and point towards that configuration file when computing the report:</p> Custom configuration file<pre><code>from ydata_profiling import ProfileReport\n\nprofile = ProfileReport(df, config_file=\"your_config.yml\")\nprofile.to_file(\"report.html\")\n</code></pre>"},{"location":"advanced_settings/changing_settings/#through-environment-variables","title":"Through environment variables","text":"<p>Any configuration setting can also be read from environment variables. For example:</p> Setting title for the report with parameters<pre><code>from ydata_profiling import ProfileReport\n\nprofile = ProfileReport(df, title=\"My Custom Profiling Report\")\n</code></pre> <p>is equivalent to setting the title as an environment variable</p> Set title through environment variables<pre><code>import os\nfrom ydata_profiling import ProfileReport\n\nos.environ(\"PROFILE_TITLE\")='My Custom Profiling Report'\n\nprofile = ProfileReport(df)\n</code></pre> <p></p>"},{"location":"advanced_settings/collaborative_data_profiling/","title":"Collaborative data profiling","text":"<p>Data quality Profiling with a Collaborative experience</p> <p>YData Fabric is a Data-Centric AI development platform. YData Fabric provides all capabilities of ydata-profiling in a hosted environment combined with a guided UI experience.</p> <p>Fabric\\'s Data Catalog provides a comprehensive and powerful tool designed to enable data professionals, including data scientists and data engineers, to manage and understand data within an organization. The Data Catalog act as a searchable repository, that captures the schema and metadata, while providing a unified view of all datasets.</p>"},{"location":"advanced_settings/collaborative_data_profiling/#profiling-in-a-data-catalog","title":"Profiling in a Data Catalog","text":""},{"location":"advanced_settings/collaborative_data_profiling/#1-built-in-connectors","title":"1. Built-in connectors","text":"<p>Fabric\\'s Data Catalog experience, provides pre-configured interfaces for a variety of data data sources. The built-in connectors simplify and expedite the data integration, while reducing developing time and enhancing data availability ensuring reliable and consistent data exchange.</p>"},{"location":"advanced_settings/collaborative_data_profiling/#2-metadata-management","title":"2. Metadata management","text":"<p>The Data Catalog captures and stores automatically the datasets metadata, providing essential information about your data types, source, last time of update, relationships among other characteristics, such as the presence of potential Personally identifiable Information (PII). It supports automated metadata ingestion from a variety of data sources, which allows to keep the catalog always up-to-date.</p> <p>3. Data profiling and relationships =================== An interactive experience that allows to drill-down in a comprehensive data profiling and relationship analysis, providing deep insights into data structure, distributions and interactions for improved data preparation.</p> <p>4. Data quality indexes =================== Access and navigate indicators and data quality statistics, such as completeness, uniqueness and consistency. This feature ensures that your teams are working with trusted, complete and reliable data while developing data initiatives.</p>"},{"location":"advanced_settings/collaborative_data_profiling/#5-collaborative","title":"5. Collaborative","text":"<p>The Data Catalog enables a collaborative experience through dataset descriptions and tags for ease of search. This fosters collaboration among team members, sharing domain knowledge and experience, and leading to better, more informed decisions.</p> <p>6. Security and Compliance =================== Through built-in connectors and flexible infrastructure enforce data access control per users and per project. YData Fabric Data Catalog helps in maintinaing regulatory compliance by identifying any sensitive data.</p> <p>Try today the Catalog experience in with Fabric Community version!</p> <p></p>"},{"location":"features/big_data/","title":"Profiling large datasets","text":"<p>By default, <code>ydata-profiling</code> comprehensively summarizes the input dataset in a way that gives the most insights for data analysis. For small datasets, these computations can be performed in quasi real-time. For larger datasets, deciding upfront which calculations to make might be required. Whether a computation scales to a large datasets not only depends on the exact size of the dataset, but also on its complexity and on whether fast computations are available. If the computation time of the profiling becomes a bottleneck, <code>ydata-profiling</code> offers several alternatives to overcome it.</p> <p>Scale in a fully managed system</p> <p>Looking for an fully managed system that is able to scale the profiling for large datasets? Sign up Fabric community for distributed data profiling.</p>"},{"location":"features/big_data/#pyspark","title":"Pyspark","text":"<p>Spark</p> <p>Minimal mode</p> <p>This mode was introduced in version v4.0.0</p> <p><code>ydata-profiling</code> now supports Spark Dataframes profiling. You can find an example of the integration here.</p> <p>Features supported: - Univariate variables' analysis - Head and Tail dataset sample - Correlation matrices: Pearson and Spearman</p> <p>Coming soon - Missing values analysis - Interactions - Improved histogram computation</p> <p>Keep an eye on the GitHub page to follow the updates on the implementation of Pyspark Dataframes support.</p>"},{"location":"features/big_data/#minimal-mode","title":"Minimal mode","text":"<p>Minimal mode</p> <p>This mode was introduced in version v2.4.0</p> <p><code>ydata-profiling</code> includes a minimal configuration file where the most expensive computations are turned off by default. This is the recommended starting point for larger datasets.</p> <pre><code>profile = ProfileReport(large_dataset, minimal=True)\nprofile.to_file(\"output.html\")\n</code></pre> <p>This configuration file can be found here: config_minimal.yaml. More details on settings and configuration are available in <code>../advanced_usage/available_settings</code>.</p>"},{"location":"features/big_data/#sample-the-dataset","title":"Sample the dataset","text":"<p>An alternative way to handle really large datasets is to use a portion of it to generate the profiling report. Several users report this is a good way to scale back the computation time while maintaining representativity.</p> <p>pandas-profiling is a nifty tool to compute descriptive statistics on a dataframe and issue warnings on columns with many missing values, high skew, high cardinality categorical values, high correlation, etc: https://t.co/57IezPW1nI demo notebook: https://t.co/JpqTO9FK1p</p>\u2014 Olivier Grisel (@ogrisel) January 11, 2018 Sampling a large dataset<pre><code># Sample 10.000 rows\nsample = large_dataset.sample(10000)\n\nprofile = ProfileReport(sample, minimal=True)\nprofile.to_file(\"output.html\")\n</code></pre> <p>The reader of the report might want to know that the profile is generated using a sample from the data. This can be done by adding a description to the report (see <code>metadata</code> for details).</p> Sample 5% of your dataset<pre><code>description = \"Disclaimer: this profiling report was generated using a sample of 5% of the original dataset.\"\nsample = large_dataset.sample(frac=0.05)\n\nprofile = sample.profile_report(dataset={\"description\": description}, minimal=True)\nprofile.to_file(\"output.html\")\n</code></pre>"},{"location":"features/big_data/#disable-expensive-computations","title":"Disable expensive computations","text":"<p>To decrease the computational burden in particularly large datasets but still maintain some information of interest that may stem from them, some computations can be filtered only for certain columns. Particularly, a list of targets can be provided to Interactions, so that only the interactions with these variables in specific are computed.</p> Disable expensive computations<pre><code>from ydata_profiling import ProfileReport\nimport pandas as pd\n\n# Reading the data\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n)\n\n# Creating the profile without specifying the data source, to allow editing the configuration\nprofile = ProfileReport()\nprofile.config.interactions.targets = [\"Name\", \"Sex\", \"Age\"]\n\n# Assigning a DataFrame and exporting to a file, triggering computation\nprofile.df = data\nprofile.to_file(\"report.html\")\n</code></pre> <p>The setting controlling this, <code>\u00ecnteractions.targets</code>, can be changed via multiple interfaces (configuration files or environment variables). For details, see <code>../advanced_usage/changing_settings</code>{.interpreted-text role=\"doc\"}.</p>"},{"location":"features/big_data/#concurrency","title":"Concurrency","text":"<p><code>ydata-profiling</code> is a project under active development. One of the highly desired features is the addition of a scalable backend such as Modin or Dask.</p> <p>Keep an eye on the GitHub page to follow the updates on the implementation of a concurrent and highly scalable backend. Specifically, development of a Spark backend is currently underway.</p>"},{"location":"features/collaborative_data_profiling/","title":"Data Catalog **","text":"<p>A collaborative experience to profile datasets &amp; relational databases</p> <p>** YData's Enterprise feature</p> <p>This feature is only available for users of YData Fabric.</p> <p>Sign-up Fabric community to try the Data catalog</p> <p>YData Fabric is a Data-Centric AI development platform. YData Fabric provides all capabilities of ydata-profiling in a hosted environment combined with a guided UI experience.</p> <p>Fabric's Data Catalog,  a scalable and interactive version of ydata-profiling, provides a comprehensive and powerful tool designed to enable data professionals, including data scientists and data engineers, to manage and understand data within an organization. The Data Catalog act as a searchable repository, that captures the schema and metadata, while providing a unified view of all datasets.</p>"},{"location":"features/collaborative_data_profiling/#profiling-in-a-data-catalog","title":"Profiling in a Data Catalog","text":""},{"location":"features/collaborative_data_profiling/#built-in-connectors","title":"Built-in connectors","text":"<p>Fabric's Data Catalog experience, provides pre-configured interfaces for a variety of data sources. The built-in connectors simplify and expedite the data integration, while reducing developing time and enhancing data availability ensuring reliable and consistent data exchange.</p>"},{"location":"features/collaborative_data_profiling/#metadata-management","title":"Metadata management","text":"<p>The Data Catalog captures and stores automatically the datasets' metadata, providing essential information about your data types, source, last time of update, relationships among other characteristics, such as the presence of potential Personally identifiable Information (PII). It supports automated metadata ingestion from a variety of data sources, which allows to keep the catalog always up-to-date.</p>"},{"location":"features/collaborative_data_profiling/#data-profiling-and-relationships","title":"Data profiling and relationships","text":"<p>An interactive experience that allows to drill-down in a comprehensive data profiling and relationship analysis, providing deep insights into data structure, distributions and interactions for improved data preparation.</p> <p> </p>"},{"location":"features/collaborative_data_profiling/#data-quality-indexes","title":"Data quality indexes","text":"<p>Access and navigate indicators and data quality statistics, such as completeness, uniqueness and consistency. This feature ensures that your teams are working with trusted, complete and reliable data while developing data initiatives.</p>"},{"location":"features/collaborative_data_profiling/#collaborative","title":"Collaborative","text":"<p>The Data Catalog enables a collaborative experience through dataset descriptions and tags for ease of search. This fosters collaboration among team members, sharing domain knowledge and experience, and leading to better, more informed decisions.</p>"},{"location":"features/collaborative_data_profiling/#security-and-compliance","title":"Security and Compliance","text":"<p>Through built-in connectors and flexible infrastructure enforce data access control per users and per project. YData Fabric Data Catalog helps in maintaining regulatory compliance by identifying any sensitive data.</p> <p>Try today the Catalog experience in with Fabric Community version!</p> <p></p>"},{"location":"features/comparing_datasets/","title":"Dataset Comparison","text":"<p>Dataframes compare support</p> <p>Profiling compare is supported from ydata-profiling version 3.5.0 onwards. Profiling compare is not (yet!) available for Spark Dataframes</p> <p><code>ydata-profiling</code> can be used to compare multiple version of the same dataset. This is useful when comparing data from multiple time periods, such as two years. Another common scenario is to view the dataset profile for training, validation and test sets in machine learning.</p> <p>The following syntax can be used to compare two datasets:</p> Comparing 2 datasets<pre><code>from ydata_profiling import ProfileReport\n\ntrain_df = pd.read_csv(\"train.csv\")\ntrain_report = ProfileReport(train_df, title=\"Train\")\n\ntest_df = pd.read_csv(\"test.csv\")\ntest_report = ProfileReport(test_df, title=\"Test\")\n\ncomparison_report = train_report.compare(test_report)\ncomparison_report.to_file(\"comparison.html\")\n</code></pre> <p>The comparison report uses the <code>title</code> attribute out of <code>Settings</code> as a label throughout. The colors are configured in <code>settings.html.style.primary_colors</code>. The numeric precision parameter <code>settings.report.precision</code> can be played with to obtain some additional space in reports.</p> <p>In order to compare more than two reports, the following syntax can be used:</p> Comparing more than 2 datasets<pre><code>from ydata_profiling import ProfileReport, compare\n\ncomparison_report = compare([train_report, validation_report, test_report])\n\n# Obtain merged statistics\nstatistics = comparison_report.get_description()\n\n# Save report to file\ncomparison_report.to_file(\"comparison.html\")\n</code></pre> <p>Note</p> <p>This functionality only ensures the support report comparison of two datasets. It is possible to obtain the statistics - the report may have formatting issues. One of the settings that can be changed is <code>settings.report.precision</code>. As a rule of thumb, the value 10 can be used for a single report and 8 for comparing two reports.</p>"},{"location":"features/custom_reports/","title":"Customizing the report","text":"<p>In some situations, a user might want to customize the appearance of the report to match personal preferences or a corporate brand. <code>ydata-profiling</code> offers two major customization dimensions:  the styling of the HTML report and the styling of the visualizations and plots contained within. </p>"},{"location":"features/custom_reports/#customizing-the-reports-theme","title":"Customizing the report's theme","text":"<p>Several aspects of the report can be customized. The table below shows the available settings:</p> Parameter Type Default Description <code>html.minify_html</code> bool <code>True</code> If <code>True</code>, the output HTML is minified using the <code>htmlmin</code> package. <code>html.use_local_assets</code> bool <code>True</code> If <code>True</code>, all assets (stylesheets, scripts, images) are stored locally. If <code>False</code>, a CDN is used for some stylesheets and scripts. <code>html.inline</code> boolean <code>True</code> If <code>True</code>, all assets are contained in the report. If <code>False</code>, then a web export is created, where all assets are stored in the '[REPORT_NAME]_assets/' directory. <code>html.navbar_show</code> boolean <code>True</code> Whether to include a navigation bar in the report <code>html.style.theme</code> string <code>None</code> Select a bootswatch theme. Available options: flatly (dark) and united (orange) <code>html.style.logo</code> string nan A base64 encoded logo, to display in the navigation bar. <code>html.style.primary_color</code> string #337ab7 The primary color to use in the report. <code>html.style.full_width</code> boolean <code>False</code> By default, the width of the report is fixed. If set to <code>True</code>, the full width of the screen is used. <p>See the available changing settings to see how to change and apply these settings.</p>"},{"location":"features/custom_reports/#customizing-the-visualizations","title":"Customizing the visualizations","text":""},{"location":"features/custom_reports/#plot-rendering-options","title":"Plot rendering options","text":"<p>A way how to pass arguments to the underlying <code>matplotlib</code> visualization engine is to use the <code>plot</code> argument when computing the profile. It is possible to change the default format of images to png (default is SVG) using the key-pair <code>image_format: \"png\"</code> and also the resolution of the images using <code>dpi: 800</code>. An example would be:</p> Customize plots rendering<pre><code>profile = ProfileReport(\n    planets,\n    title=\"Pandas Profiling Report\",\n    explorative=True,\n    plot={\"dpi\": 200, \"image_format\": \"png\"},\n)\n</code></pre>"},{"location":"features/custom_reports/#pie-charts","title":"Pie charts","text":"<p>Pie charts are used to plot the frequency of categories in categorical (or boolean) features. By default, a feature is considered as categorical if it does not have more than 10 distinct values. This threshold can be configured with the <code>plot.pie.max_unique</code> setting.</p> Control pie charts frequency<pre><code>profile = ProfileReport(pd.DataFrame([\"a\", \"b\", \"c\"]))\n# Changing the *max_unique* threshold to 2 will make feature non-categorical\nprofile.config.plot.pie.max_unique = 2\n</code></pre> <p>If the feature is not considered as categorical, the pie chart will not be displayed. All pie charts can therefore be removed by setting: <code>plot.pie.max_unique = 0</code>.</p> <p>The pie chart's colors can be configured to any recognised matplotlib colour <code>plot.pie.colors</code> setting. </p> Control pie charts colors<pre><code>profile = ProfileReport(pd.DataFrame([1, 2, 3]))\nprofile.config.plot.pie.colors = [\"gold\", \"b\", \"#FF796C\"]\n</code></pre>"},{"location":"features/custom_reports/#colour-palettes","title":"Colour palettes","text":"<p>The palettes used in visualizations like correlation matrices and missing values overview can also be customized via the <code>plot</code> argument. To customize the palette used by the correlation matrix, use the <code>correlation</code> key:</p> Changing visualizations color palettes<pre><code>  from ydata_profiling import ProfileReport\n\n  profile = ProfileReport(\n      df,\n      title=\"Pandas Profiling Report\",\n      explorative=True,\n      plot={\"correlation\": {\"cmap\": \"RdBu_r\", \"bad\": \"#000000\"}},\n  )\n\nSimilarly, the palette for *Missing values* can be changed using ``missing`` argument:\n\n``` python linenums=\"1\" python\n\n  from ydata_profiling import ProfileReport\n\n  profile = ProfileReport(\n      df,\n      title=\"Pandas Profiling Report\",\n      explorative=True,\n      plot={\"missing\": {\"cmap\": \"RdBu_r\"}},\n  )\n</code></pre> <p><code>ydata-profiling</code> accepts all <code>cmap</code> values (colormaps) accepted by <code>matplotlib</code>. The list of available colour maps can be accessed here. Alternatively, it is possible to create custom palettes.</p> <p></p>"},{"location":"features/metadata/","title":"Dataset description &amp; Metadata","text":""},{"location":"features/metadata/#dataset-description","title":"Dataset description","text":"<p>When sharing reports with coworkers or publishing online, it might be important to include metadata of the dataset, such as author, copyright holder or descriptions. <code>ydata-profiling</code> allows complementing a report with that information. Inspired by schema.org\\'s Dataset, the currently supported properties are description, creator, author, url, copyright_year and copyright_holder.</p> <p>The following example shows how to generate a report with a description, copyright_holder copyright_year, creator and url. In the generated report, these properties are found in the Overview, under About.</p> Add profile report description<pre><code>report = df.profile_report(\n    title=\"Masked data\",\n    dataset={\n        \"description\": \"This profiling report was generated using a sample of 5% of the original dataset.\",\n        \"copyright_holder\": \"StataCorp LLC\",\n        \"copyright_year\": 2020,\n        \"url\": \"http://www.stata-press.com/data/r15/auto2.dta\",\n    },\n)\n\nreport.to_file(Path(\"stata_auto_report.html\"))\n</code></pre>"},{"location":"features/metadata/#column-descriptions","title":"Column descriptions","text":"<p>In addition to providing dataset details, often users want to include column-specific descriptions when sharing reports with team members and stakeholders. <code>ydata-profiling</code> supports creating these descriptions, so that the report includes a built-in data dictionary. By default, the descriptions are presented in the Overview section of the report, next to each variable.</p> Generate a report with per-variable descriptions<pre><code>profile = df.profile_report(\n    variables={\n        \"descriptions\": {\n            \"files\": \"Files in the filesystem, # variable name: variable description\",\n            \"datec\": \"Creation date\",\n            \"datem\": \"Modification date\",\n        }\n    }\n)\n\nprofile.to_file(report.html)\n</code></pre> <p>Alternatively, column descriptions can be loaded from a JSON file:</p> dataset_column_definition.json<pre><code>{\n    column name 1: column 1 definition,\n    column name 2: column 2 definition\n}\n</code></pre> Generate a report with descriptions per variable from a JSON definitions file<pre><code>import json\nimport pandas as pd\nimport ydata_profiling\n\ndefinition_file = dataset_column_definition.json\n\n# Read the variable descriptions\nwith open(definition_file, r) as f:\n    definitions = json.load(f)\n\n# By default, the descriptions are presented in the Overview section, next to each variable\nreport = df.profile_report(variable={\"descriptions\": definitions})\n\n# We can disable showing the descriptions next to each variable\nreport = df.profile_report(\n    variable={\"descriptions\": definitions}, show_variable_description=False\n)\n\nreport.to_file(\"report.html\")\n</code></pre>"},{"location":"features/metadata/#dataset-schema","title":"Dataset schema","text":"<p>In addition to providing dataset details, users often want to include set type schemas. This is particularly important when integrating <code>ydata-profiling</code> generation with the information already in a data catalog. When using <code>ydata-profiling</code> ProfileReport, users can set the type_schema property to control the generated profiling data types. By default, the <code>type_schema</code> is automatically inferred with visions.</p> Set the variable type schema to Generate the profile report<pre><code>import json\nimport pandas as pd\n\nfrom ydata_profiling import ProfileReport\nfrom ydata_profiling.utils.cache import cache_file\n\nfile_name = cache_file(\n    \"titanic.csv\",\n    \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\",\n)\ndf = pd.read_csv(file_name)\n\ntype_schema = {\"Survived\": \"categorical\", \"Embarked\": \"categorical\"}\n\n# We can set the type_schema only for the variables that we are certain of their types. All the other will be automatically inferred.\nreport = ProfileReport(df, title=\"Titanic EDA\", type_schema=type_schema)\n\nreport.to_file(\"report.html\")\n</code></pre> <p></p>"},{"location":"features/pii_identification_management/","title":"Personally identifiable information (PII) identification &amp; management **","text":"<p>** YData's Enterprise feature</p> <p>This feature is only available for users of YData Fabric.</p> <p>Sign-up Fabric community and start your journey into data management with automated PII identification.</p> <p>Personal Identifiable Information (PII) refers to any information that can be used to identify an individual. This includes but is not limited to, names, addresses, phone numbers, social security numbers, email addresses,  and financial information. PII is crucial in today's digital age, where data is extensively collected, stored,  and processed.</p> <p>YData Fabric Data Catalog, a scalable and interactive version of ydata-profiling,  integrates into the data profiling experience, an advanced machine learning solutions based on a Named Entity Recognition (NER) model combine with traditional rule-based patterns identification, allowing to efficiently detect PII. </p> <p> See Fabric's Data Catalog PII identification in action.</p>"},{"location":"features/pii_identification_management/#why-fabric-catalog-automated-pii-identification","title":"Why Fabric Catalog automated PII identification?","text":"<p>The relevance of automating the identification of PII lies in the need to protect individuals' privacy and comply with various data protection regulations. Mishandling or unauthorized access to PII can lead to severe consequences such as identity theft, financial fraud, and breaches of privacy. With the increasing volume of data generated manual identification of PII becomes impractical and error-prone.</p> <p>Additionally, having a robust PII management solution is essential for organizations to establish and maintain  a secure approach to handling sensitive information, fostering trust and adhering to legal requirements.</p>"},{"location":"features/pii_identification_management/#why-fabric-to-manage-dataset-pii-identification","title":"Why Fabric to manage dataset PII identification","text":"<p>Besides automated PII identification, Fabric Catalog offers several key benefits in the content of data governance, privacy compliance and overall data management, through automated data profiling and metadata management:</p>"},{"location":"features/pii_identification_management/#compliance-with-privacy-regulations","title":"Compliance with Privacy Regulations:","text":"<p>Many countries and regions have stringent data protection regulations (such as GDPR, CCPA, or HIPAA)  that require organizations to handle PII responsibly. A dedicated platform ensures that PII is correctly classified,  helping organizations comply with legal requirements and avoid potential penalties.</p>"},{"location":"features/pii_identification_management/#data-profiling-for-accuracy","title":"Data Profiling for Accuracy:","text":"<p>Data profiling involves analyzing and understanding the structure and content of data. By incorporating data profiling capabilities into the platform, organizations can ensure accurate identification and classification of PII. This helps in maintaining the integrity of data and reduces the risk of misclassifications.</p>"},{"location":"features/pii_identification_management/#efficient-management-of-pii","title":"Efficient Management of PII:","text":"<p>As the volume of data continues to grow, manually managing and editing PII classifications becomes impractical.  A platform streamlines this process, making it more efficient and reducing the likelihood of errors.  It allows organizations to keep track of PII across various datasets and systems.</p>"},{"location":"features/pii_identification_management/#facilitating-data-governance","title":"Facilitating Data Governance:","text":"<p>Data governance involves establishing policies and processes to ensure high data quality, security, and compliance.  A PII management solution enhances data governance efforts by providing a centralized hub for overseeing PII classifications, metadata, and related policies.</p> <p></p>"},{"location":"features/profile_values/","title":"Accessing profile files","text":""},{"location":"features/profile_values/#json-output-structure","title":"Json output structure","text":""},{"location":"features/profile_values/#univariate-variables-statistics-through-description_set","title":"Univariate variables statistics through description_set","text":""},{"location":"features/profile_values/#correlation-matrices-through-description_set","title":"Correlation matrices through description_set","text":""},{"location":"features/sensitive_data/","title":"Handling sensitive data","text":"<p>In certain data-sensitive contexts (for instance, private health records), sharing a report that includes a sample would violate privacy constraints. The following configuration shorthand groups together various options so that only aggregate information is provided in the report and no individual records are shown:</p> <pre><code>report = df.profile_report(sensitive=True)\n</code></pre> <p>Additionally, <code>ydata-profiling</code> does not send data to external services, making it suitable for private data.</p>"},{"location":"features/sensitive_data/#sample-and-duplicates","title":"Sample and duplicates","text":"<p>Explicitly showing a dataset\\'as sample and duplicate rows can be disabled, to guarantee the report does not directly leak any data:</p> <pre><code>report = df.profile_report(duplicates=None, samples=None)\n</code></pre> <p>Alternatively, it is possible to still show a sample but The following snippet demonstrates how to generate the report but using mock/synthetic data in the dataset sample sections. Note that the <code>name</code> and <code>caption</code> keys are optional.</p> Generate profiling with sensitive data: mocked sample<pre><code># Replace with the sample you'd like to present in the report (can be from a mock or synthetic data generator)\nsample_custom_data = pd.DataFrame()\nsample_description = \"Disclaimer: the following sample consists of synthetic data following the format of the underlying dataset.\"\n\nreport = df.profile_report(\n    sample={\n        \"name\": \"Mock data sample\",\n        \"data\": sample_custom_data,\n        \"caption\": sample_description,\n    }\n)\n</code></pre> <p>Warning</p> <p>Be aware when using <code>pandas.read_csv</code> with sensitive data such as phone numbers. pandas' type guessing will by default coerce phone numbers such as <code>0612345678</code> to numeric. This leads to information leakage through aggregates (min, max, quantiles). To prevent this from happening, keep the string representation.</p> <pre><code>pd.read_csv(\"filename.csv\", dtype={\"phone\": str})\n</code></pre> <p>Note that the type detection is hard. That is why visions, a type system to help developers solve these cases, was developed.</p>"},{"location":"features/sensitive_data/#automated-pii-classification-management","title":"Automated PII classification &amp; management","text":"<p>You can find more details about this feature here.</p> <p></p>"},{"location":"features/time_series_datasets/","title":"Time-Series data","text":"<p><code>ydata-profiling</code> can be used for a quick Exploratory Data Analysis on time-series data. This is useful for a quick understanding on the behaviour of time dependent variables regarding behaviours such as time plots, seasonality, trends, stationary and data gaps.</p> <p>Combined with the profiling reports compare, you're able to compare the evolution and data behaviour through time, in terms of time-series specific statistics such as PACF and ACF plots. It also provides the identification of gaps in the time series, caused either by missing values or by entries missing in the time index.</p> <p>Time series EDA tutorial</p> <p>Do you want to learn how to interpret the time-series profiling check out blog content here. </p> <p>You can find the a [otebook with the full code in our examples folder.</p> <pre><code>\n</code></pre> <p> </p> Time-series profiling report"},{"location":"features/time_series_datasets/#profiling-time-series-dataset","title":"Profiling time-series dataset","text":"<p>The following syntax can be used to generate a profile under the assumption that the dataset includes time dependent features:</p> Setting the configurations for time-series profiling<pre><code>import pandas as pd\n\nfrom ydata_profiling.utils.cache import cache_file\nfrom ydata_profiling import ProfileReport\n\nfile_name = cache_file(\n    \"pollution_us_2000_2016.csv\",\n    \"https://query.data.world/s/mz5ot3l4zrgvldncfgxu34nda45kvb\",\n)\n\ndf = pd.read_csv(file_name, index_col=[0])\n\n# Filtering time-series to profile a single site\nsite = df[df[\"Site Num\"] == 3003]\n\n#Enable tsmode to True to automatically identify time-series variables\n#Provide the column name that provides the chronological order of your time-series\nprofile = ProfileReport(df, tsmode=True, sortby=\"Date Local\", title=\"Time-Series EDA\")\n\nprofile.to_file(\"report_timeseries.html\")\n</code></pre> <p>To enable a time-series report to be generated <code>ts_mode</code> needs to be set to <code>True</code>. If <code>True</code> the variables that have temporal dependence will be automatically identified based on the presence of autocorrection. The time-series report uses the <code>sortby</code> attribute to order the dataset. If not provided it is assumed that the dataset is already ordered. You can set up the correlation level to detect to apply the time-series  validation by setting the x configuration. </p>"},{"location":"features/time_series_datasets/#warnings-and-validations","title":"Warnings and validations","text":"<p>Specific to time-series analysis, 2 new warnings were added to the <code>ydata-profiling</code> warnings family: NON_STATIONARY and SEASONAL.</p>"},{"location":"features/time_series_datasets/#stationarity","title":"Stationarity","text":"<p>In the realm of time-series analysis, a stationary time-series is a dataset  where statistical properties, such as mean, variance, and autocorrelation,  remain constant over time. This property is essential for many time-series  forecasting and modeling techniques because they often assume that the underlying  data is stationary. Stationarity simplifies the modeling process by making it easier to detect patterns and trends.</p> <p><code>ydata-profiling</code> stationary warning is based on an Augmented Dickey-Fuller(ADF) test. Nevertheless, you should always combine the output of this warning with a visual inspection to your time-series behaviour and search for variance of the  rolling statistics analysis. </p>"},{"location":"features/time_series_datasets/#seasonality","title":"Seasonality","text":"<p>A seasonal time-series is a specific type of time-series data that exhibits recurring patterns or fluctuations at regular intervals. These patterns are known as seasonality and are often observed in data associated with yearly, monthly, weekly, or daily cycles. Seasonal time-series data can be challenging to model accurately without addressing the underlying seasonality.</p> <p><code>ydata-profiling</code> seasonality warning is based on an Augmented Dickey-Fuller(ADF) test. Nevertheless, you should always combine the output of this warning with a seasonal decomposition PACF and ACF plots (also computed in your time-series profiling).</p>"},{"location":"features/time_series_datasets/#time-series-missing-gaps","title":"Time-series missing gaps","text":"Time-series missing data visualization <p>As a data scientist, one of the critical aspects of working with time-series data is understanding and analyzing time-series gaps. Time-series gaps refer to the intervals within your time-series data where observations are missing or incomplete. While these gaps might seem like inconveniences, they hold valuable information and can significantly impact the quality and reliability of your analyses and predictions. </p> <p><code>ydata-profiling</code> automated identification of potential time-series gaps is based on time intervals analysis. By analyzing the time intervals between data points, the gaps are expected to be reflected as larger intervals in the distribution. </p> <p>You can set up the configuration and intervals for the missing gaps identification here.</p>"},{"location":"features/time_series_datasets/#customizing-time-series-variables","title":"Customizing time-series variables","text":"<p>In some cases you might be already aware of what variables are expected to be time-series or, perhaps, you just want to ensure that the variables that you want to analyze as time-series are profiled as such:</p> Setting what variables are time-series<pre><code>import pandas as pd\n\nfrom ydata_profiling.utils.cache import cache_file\nfrom ydata_profiling import ProfileReport\n\nfile_name = cache_file(\n    \"pollution_us_2000_2016.csv\",\n    \"https://query.data.world/s/mz5ot3l4zrgvldncfgxu34nda45kvb\",\n)\n\ndf = pd.read_csv(file_name, index_col=[0])\n\n# Filtering time-series to profile a single site\nsite = df[df[\"Site Num\"] == 3003]\n\n# Setting what variables are time series\ntype_schema = {\n    \"NO2 Mean\": \"timeseries\",\n    \"NO2 1st Max Value\": \"timeseries\",\n    \"NO2 1st Max Hour\": \"timeseries\",\n    \"NO2 AQI\": \"timeseries\",\n}\n\nprofile = ProfileReport(\n    df,\n    tsmode=True,\n    type_schema=type_schema,\n    sortby=\"Date Local\",\n    title=\"Time-Series EDA for site 3003\",\n)\n\nprofile.to_file(\"report_timeseries.html\")\n</code></pre> <p>For more questions and suggestions around time-series analysis reach us out at the Data-Centric AI community.</p> <p></p>"},{"location":"getting-started/concepts/","title":"Concepts","text":"<p>Text/corpus data - your input is needed!</p> <p><code>ydata-profiling</code> team is considering the support of a new set of features for corpus data and we want to hear from you! We're particularly interested in understanding why you think these features would be useful, and your input will help us prioritize and refine this development.</p> <p>\ud83d\udc49 Upvote [add here link for the request form]</p>"},{"location":"getting-started/concepts/#data-structures-supported","title":"Data Structures supported","text":"<p>The profiling offers comprehensive insights into various types of data, including tabular, time-series text and image data. </p> <ul> <li>Tabular data: when dealing with tabular data, such as spreadsheets or databases, the profiling provides valuable statistics on data distribution, central tendencies, and categorical variable frequencies. It identifies multivariate relations such as correlations and interactions in a visual manner. It also identifies missing data.  </li> <li>Time-series data: when dealing with data with temporal dimensions, the profiling extends its capabilities to capture trends, seasonality, cyclic patterns and missing data gaps.  It can reveal information about data volatility, periodicity, and anomalies, facilitating a deeper understanding of time-dependent trends.</li> <li>Text:  when it comes to text data, such as strings or documents, the profiling offers insightful statistics on the distribution of word frequencies, common phrases, and unique words. </li> </ul>"},{"location":"getting-started/concepts/#data-types","title":"Data types","text":"<p>Types, when going beyond the logical data types such as integer, floats, etc,  are a powerful abstraction for effective data analysis, allowing analysis under higher level lenses. <code>ydata-profiling</code> is backed by a powerful type system developed specifically for data analysis: <code>visions &lt;https://github.com/dylan-profiler/visions&gt;</code>_. Currently, <code>ydata-profiling</code> recognizes the following types:</p> <ul> <li>Boolean</li> <li>Numerical</li> <li>Date (and Datetime)</li> <li>Categorical</li> <li>Time-series</li> <li>URL</li> <li>Path</li> <li>File</li> <li>Image</li> </ul> <p>Appropriate typesets can both improve the overall expressiveness and reduce the complexity of the analysis/code.  User customized summarizations and type definitions are fully supported, with PRs supporting new data types for specific use cases more than welcome. For reference, you can check the implementation of <code>ydata-profiling</code>'s default typeset here.</p>"},{"location":"getting-started/concepts/#data-quality-alerts","title":"Data quality alerts","text":"Data quality warnings <p>Alerts section in the NASA Meteorites dataset's report. Some alerts include numerical indicators. </p> <p>The Alerts section of the report includes a comprehensive and automatic list of potential data quality issues. Although useful, the decision on whether an alert is in fact a data quality issue always requires domain validation. Some warnings refer to a specific column, others refer to inter-column relationships and others are dataset-wide. The table below lists all possible data quality alerts and their meanings.</p> Alert Description <code>Constant</code> Column only contains one value <code>Zeros</code> Column only contains zeros <code>High Correlation</code> Correlations (either Spearman, Cramer, Pearson, Kendall, \ud835\udf19k) are above the warning threshold (configurable). <code>High Cardinality</code> Whether the column has more than 50 distinct values. Threshold is configurable. <code>Imbalance</code> Column is highly imbalanced. Threshold is configurable. <code>Skewness</code> Column's univariate distribution presents skewness. Threshold value is configurable. <code>Missing Values</code> Column has missing values <code>Infinite Values</code> Column has infinite values (either <code>np.inf</code> or <code>-np.inf</code>) <code>Unique Values</code> All values of the column are unique (count of unique values equals column's length) <code>Seasonal</code> Column has seasonal pattern <code>Non Stationary</code> Column is a time-series non-stationary <code>Date</code> Column (likely/mostly) contains Date or Datetime records <code>Uniform</code> Column follows a uniform distribution (Chi-squared test score &gt; 0.999, threshold score is configurable) <code>Constant length</code> For strings/date/datetimes columns whose entries all have the same length <code>Rejected</code> Variable has mixed types or is constant (thus not suitable for meaningful analysis) <code>Unsupported</code> Column can't be analysed (type is not supported, has mixed types, has <code>lists</code>/<code>dicts</code>/<code>tuples</code>, is empty, wrongly formatted) <code>Duplicates</code> Dataset-level warning signaling the presence of more than 10 duplicated records. <code>Empty</code> Dataset-level warning signaling there's no data to be analysed. <p>Information on the default values and the specific parameters/thresholds used in the computation of these alerts,  as well as settings to disable specific ones, can be consulted in the documentation.</p>"},{"location":"getting-started/concepts/#univariate-profiling","title":"Univariate profiling","text":"Univariate profiling metrics and visualization <p>This section provides a comprehensive overview of individual variables within a given dataset, this feature is particularly useful for exploratory data analysis (EDA) as it automatically calculated detailed statistics, visualizations, and insights for each variable in the dataset. It offers information such as data type, missing values, unique values, basic descriptive statistics , histogram plots, and distribution plots. This allows data analysts and scientists to quickly understand the characteristics of each variable, identify potential data quality issues, and gain initial insights into the data's distribution and variability. </p> <p>For more details about the different metrics and visualizations check the Univariate section details page. </p>"},{"location":"getting-started/concepts/#multivariate-profiling","title":"Multivariate profiling","text":"Multivariate profiling metrics and visualization <p>This section provides essentials insights into the relationships between variables through correlations matrices and interactions.  The correlation view computes and presents correlation coefficients between pairs of numerical variables, helping to identify potential linear relationships. This assists data analysts and scientists in understanding how variables change together and highlights possible multicollinearity issues.</p> <p>On the other hand, the interactions section goes beyond correlation by exploring potential nonlinear relationships and interactions between variables, providing a more comprehensive understanding of how variables interact with one another.  This can be crucial in identifying hidden patterns that might not be captured through traditional correlation analysis.</p> <p>Check the section about interactions configuration and correlation matrix metrics for more details.  </p>"},{"location":"getting-started/concepts/#missing-data","title":"Missing data","text":"<p>This section offers valuable insights into the presence and distribution of missing data within a dataset. It can be particularly helpful for data preprocessing and quality assesment as provides a comprehensive summary of missing values across variables, indicating the percentage of missing data for each variable. Additionally, it displays a visual representation of missing data patterns through bar plots and heatmaps,  allowing users to quickly identify which variables have the most significant amount of missing information.</p> <p>Check how you can configure your missing data visualization. </p>"},{"location":"getting-started/concepts/#outliers","title":"Outliers **","text":"Outliers identification <p>This section provides a comprehensive profiling over the potential dataset outliers. You can validate and observe outliers presence and deviation from the general distribution of numerical variables based on observed variance.  The identification of outliers allows the data analyst or scientist to assess whether they are genuine data anomalies or erroneous entries, allowing for informed decisions on whether to retain, transform, or exclude these points in further analyses.</p> <p>Feature limited to user of the cloud hosted solution.</p>"},{"location":"getting-started/concepts/#preview-data","title":"Preview data","text":"<p>For a quick overview of the data, ydata-profiling provides the following sections that can be easily configure by the user: - First n records of a given dataset - Last n records of a given dataset - A table containing observed duplicates (exact matches)</p> <p></p>"},{"location":"getting-started/examples/","title":"Examples","text":"<p>The following example reports showcase the potentialities of the package across a wide range of dataset and data types:</p> <ul> <li>Census     Income     (US Adult Census data relating income with other demographic     properties)</li> <li>NASA     Meteorites     (comprehensive set of meteorite landing - object properties and     locations)      </li> <li>Titanic     (the \\\"Wonderwall\\\" of datasets)      </li> <li>NZA     (open data from the Dutch Healthcare Authority)</li> <li>Stata     Auto     (1978 Automobile data, Stata format)</li> <li>Colors     (a simple colors dataset)</li> <li>Vektis     (Vektis Dutch Healthcare data)</li> <li>UCI Bank     Dataset     (marketing dataset from a bank)</li> <li>Russian     Vocabulary     (100 most common Russian words, showcasing unicode text analysis)</li> <li>Website     Inaccessibility     (website accessibility analysis, showcasing support for URL data)</li> <li>Orange     prices     and Coal     prices     (simple pricing evolution datasets, showcasing the theming options)</li> <li>USA Air     Quality     (Time-series air quality dataset EDA example)</li> <li>HCC     (Open dataset from healthcare, showcasing compare between two sets     of data, before and after preprocessing)</li> </ul> <p></p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>You can install using the <code>pip</code> package manager by running:</p> <p><pre><code>    pip install -U ydata-profiling\n</code></pre> If you are in a notebook (locally, LambdaLabs, Google Colab or Kaggle), you can run:</p> <p><pre><code>    import sys\n    !{sys.executable} -m pip install -U ydata-profiling[notebook]\n    !pip install jupyter-contrib-nbextensions\n</code></pre> Afterwards you can run the following command</p> <p><pre><code>    !jupyter nbextension enable --py widgetsnbextension\n</code></pre> You may have to restart the kernel or runtime for the package to work.</p>"},{"location":"getting-started/installation/#using-conda","title":"Using conda","text":"<p>ydata-profiling through Conda</p> <p>A new conda environment containing the module can be created via: </p> <pre><code>    conda env create -n ydata-profiling\n    conda activate ydata-profiling\n    conda install -c conda-forge ydata-profiling\n</code></pre> <p>Tip</p> <pre><code>Don't forget to specify the ``conda-forge`` channel.       \nOmitting it **will not** lead to an error, as an outdated package lives on the ``main`` channel and will be installed. See :doc:`../support_contrib/common_issues` for details.\n</code></pre>"},{"location":"getting-started/installation/#widgets-in-jupyter-notebooklab","title":"Widgets in Jupyter Notebook/Lab","text":"<p>For the Jupyter widgets extension (used for progress bars and the interactive widget-based report) to work, you might need to install and activate the corresponding extensions.  This can be done via <code>pip</code>: </p> <pre><code>  pip install ydata-profiling[notebook]\n  jupyter nbextension enable --py widgetsnbextension\n</code></pre> <p>Or via <code>conda</code>: <pre><code>  conda install -c conda-forge ipywidgets\n</code></pre></p> <p>In most cases, this will also automatically configure Jupyter Notebook and Jupyter Lab (<code>&gt;=3.0</code>). For older versions of both or in more complex environment configurations, refer to the official ipywidgets documentation.</p>"},{"location":"getting-started/installation/#from-source","title":"From source","text":"<p>Download the source code by cloning the repository or by clicking on Download ZIP. Install it by navigating to the uncompressed directory and running:</p> <pre><code>   python setup.py install\n</code></pre> <p>This can also be done via the following one-liner: </p> <pre><code>  pip install https://github.com/ydataai/ydata-profiling/archive/master.zip\n</code></pre>"},{"location":"getting-started/installation/#extras","title":"Extras","text":"<p>The package declares some \"extras\", sets of additional dependencies.</p> <ul> <li><code>[notebook]</code>: support for rendering the report in Jupyter notebook widgets.</li> <li><code>[unicode]</code>: support for more detailed Unicode analysis, at the expense of additional disk space.</li> <li><code>[pyspark]</code>: support for pyspark engine to run the profile on big datasets</li> </ul> <p>Install these with e.g. <pre><code>  pip install -U ydata-profiling[notebook,unicode, pyspark]\n</code></pre></p> <p></p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Start by loading your pandas <code>DataFrame</code> as you normally would, e.g. by using:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\ndf = pd.DataFrame(np.random.rand(100, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n</code></pre> <p>To generate the standard profiling report, merely run:</p> <pre><code>profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n</code></pre>"},{"location":"getting-started/quickstart/#using-inside-jupyter-notebooks","title":"Using inside Jupyter Notebooks","text":"<p>There are two interfaces to consume the report inside a Jupyter notebook (see animations below): through widgets and through an embedded HTML report.</p> <p></p> <p>This is achieved by simply displaying the report as a set of widgets. In a Jupyter Notebook, run:</p> <pre><code>profile.to_widgets()\n</code></pre> <p>The HTML report can be directly embedded in a cell in a similar fashion:</p> <pre><code>profile.to_notebook_iframe()\n</code></pre> <p></p>"},{"location":"getting-started/quickstart/#exporting-the-report-to-a-file","title":"Exporting the report to a file","text":"<p>To generate a HTML report file, save the <code>ProfileReport</code> to an object and use the <code>to_file()</code> function:</p> <pre><code>profile.to_file(\"your_report.html\")\n</code></pre> <p>Alternatively, the report's data can be obtained as a JSON file:</p> Save your profile report as a JSON file<pre><code># As a JSON string\njson_data = profile.to_json()\n\n# As a file\nprofile.to_file(\"your_report.json\")\n</code></pre>"},{"location":"getting-started/quickstart/#command-line-usage","title":"Command line usage","text":"<p>For standard formatted CSV files (which can be read directly by pandas without additional settings), the <code>ydata_profiling</code> executable can be used in the command line. The example below generates a report named Example Profiling Report, using a configuration file called <code>default.yaml</code>, in the file <code>report.html</code> by processing a <code>data.csv</code> dataset.</p> <pre><code>ydata_profiling --title \"Example Profiling Report\" --config_file default.yaml data.csv report.html\n</code></pre> <p>Information about all available options and arguments can be viewed through the command below. The CLI allows defining input and output filenames, setting a custom report title, specifying <code>a configuration file for custom behaviour &lt;../advanced_usage/changing_settings&gt;</code>{.interpreted-text role=\"doc\"} and control other advanced aspects of the experience.</p> <pre><code>ydata_profiling -h\n</code></pre> <p> </p> Options available in the CLI"},{"location":"getting-started/quickstart/#deeper-profiling","title":"Deeper profiling","text":"<p>The contents, behaviour and appearance of the report are easily customizable. The example below used the explorative mode, a lightweight data profiling option.</p> <pre><code>profile = ProfileReport(df, title=\"Profiling Report\", explorative=True)\n</code></pre> <p>On the CLI utility <code>ydata_profiling</code>, this mode can be activated with the <code>-e</code> flag. Learn more about configuring <code>ydata-profiling</code> on the <code>../advanced_usage/available_settings</code>.</p> <p></p>"},{"location":"integrations/bytewax/","title":"Profiling for streaming data","text":""},{"location":"integrations/bytewax/#about-bytewax","title":"About Bytewax","text":"<p>Bytewax is an OSS stream processing framework designed specifically for Python developers.   It allows users to build streaming data pipelines and real-time applications with capabilities similar to Flink, Spark, and Kafka Streams, while providing a friendly and familiar interface and 100% compatibility with the Python ecosystem.</p>"},{"location":"integrations/bytewax/#stream-processing-with-bytewax-and-ydata-profiling","title":"Stream processing with Bytewax and ydata-profiling","text":"<p>Data Profiling is key to a successful start of any machine learning task, and refers to the step of thoroughly understanding our data: its structure, behavior, and quality.                              In a nutshell, data profiling involves analyzing aspects related to  the data's format and basic descriptors (e.g., number of samples,  number/types of features, duplicate values), its intrinsic         characteristics (such as the presence of missing data or imbalanced  features), and other complicating factors that may arise during data collection or processing (e.g., erroneous values or inconsistent   features).                                                          </p> <p>Package versions</p> <p>The integration with bytewax is available for ydata-profiling with any version &gt;=3.0.0                                                </p>"},{"location":"integrations/bytewax/#simulating-a-streaming","title":"Simulating a streaming","text":"<p>The below code serves to mimic a stream of data. This not require when streaming data sources are available.</p> Imports<pre><code>from datetime import datetime, timedelta, timezone\n\nfrom bytewax.dataflow import Dataflow\nfrom bytewax.connectors.stdio import StdOutput\nfrom bytewax.connectors.files import CSVInput\nfrom bytewax.testing import run_main\n</code></pre> <p>Then, we define our dataflow object. Afterwards, we will use a stateless map method where we pass in a function to convert the string to a datetime object and restructure the data to the format (device_id, data). The map method will make the change to each data point in a stateless way. The reason we have modified the shape of our data is so that we can easily group the data in the next steps to profile data for each device separately rather than for all the devices simultaneously.</p> Setup a data stream<pre><code>flow = Dataflow()\nflow.input(\"simulated_stream\", CSVInput(\"/content/iot_telemetry_data_1000\"))\n\n# parse timestamp\ndef parse_time(reading_data):\n    reading_data[\"ts\"] = datetime.fromtimestamp(float(reading_data[\"ts\"]), timezone.utc)\n    return reading_data\n\nflow.map(parse_time)\n\n# remap format to tuple (device_id, reading_data)\nflow.map(lambda reading_data: (reading_data[\"device\"], reading_data))\n</code></pre> <p>Now we will take advantage of the stateful capabilities of bytewax to gather data for each device over a duration of time that we have defined. ydata-profiling expects a snapshot of the data over time, which makes the window operator the perfect method to use to do this.</p> <p>In ydata-profiling, we are able to produce summarizing statistics for a dataframe which is specified for a particular context. For instance, in this example, we can produce snapshots of data referring to each IoT device or to particular time frames:</p>"},{"location":"integrations/bytewax/#profile-streaming-snapshots","title":"Profile streaming snapshots","text":"Profiling the different data snapshots<pre><code>from bytewax.window import EventClockConfig, TumblingWindow\n\n# This is the accumulator function, and outputs a list of readings\ndef acc_values(acc, reading):\n    acc.append(reading)\n    return acc\n\n# This function instructs the event clock on how to retrieve the\n# event's datetime from the input.\ndef get_time(reading):\n    return reading[\"ts\"]\n\n\n# Configure the `fold_window` operator to use the event time.\ncc = EventClockConfig(get_time, wait_for_system_duration=timedelta(seconds=30))\n\n# And a tumbling window\nalign_to = datetime(2020, 1, 1, tzinfo=timezone.utc)\nwc = TumblingWindow(align_to=align_to, length=timedelta(hours=1))\n\nflow.fold_window(\"running_average\", cc, wc, list, acc_values)\n\nflow.inspect(print)\n</code></pre> <p>After the snapshots are defined, leveraging ydata-profiling is as simple as calling the ProfileReport for each of the dataframes we would like to analyze:</p> <pre><code>import pandas as pd\nfrom ydata_profiling import ProfileReport\n\n\ndef profile(device_id__readings):\n    print(device_id__readings)\n    device_id, readings = device_id__readings\n    start_time = (\n        readings[0][\"ts\"]\n        .replace(minute=0, second=0, microsecond=0)\n        .strftime(\"%Y-%m-%d %H:%M:%S\")\n    )\n    df = pd.DataFrame(readings)\n    profile = ProfileReport(\n        df, tsmode=True, sortby=\"ts\", title=f\"Sensor Readings - device: {device_id}\"\n    )\n\n    profile.to_file(f\"Ts_Profile_{device_id}-{start_time}.html\")\n    return f\"device {device_id} profiled at hour {start_time}\"\n\n\nflow.map(profile)\n</code></pre> <p>In this example we are writing the images out to local files as part of a function in a map method. These could be reported out via a messaging tool, or we could save them to some remote storage in the future. Once the profile is complete, the dataflow expects some output so we can use the built-in [StdOutput]{.title-ref} to print the device that was profiled and the time it was profiled at that was passed out of the profile function in the map step:</p> <pre><code>flow.output(\"out\", StdOutput())\n</code></pre> <p>There are multiple ways to execute Bytewax dataflows. In this example, we use the same local machine, but Bytewax can also run on multiple Python processes, across multiple hosts, in a Docker container, using a Kubernetes cluster, and more. In this example, we\\'ll continue with a local setup, but we encourage you to check waxctl which manages Kubernetes dataflow deployments once your pipeline is ready to transition to production.</p> <p>Assuming we are in the same directory as the file with the dataflow definition, we can run it using:</p> <pre><code>python -m bytewax.run ydata-profiling-streaming:flow\n</code></pre> <p>We can then use the profiling reports to validate the data quality, check for changes in schemas or data formats, and compare the data characteristics between different devices or time windows.</p> <p>We can further leverage the comparison report functionality that highlights the differences between two data profiles in a straightforward manner, making it easier for us to detect important patterns that need to be investigated or issues that have to be addressed:</p> Comparing different streams<pre><code>#Generate the profile for each stream\nsnapshot_a_report = ProfileReport(df_a, title=\"Snapshot A\")\nsnapshot_b_report = ProfileReport(df_b, title=\"Snapshot B\")\n\n#Compare the generated profiles\ncomparison_report = snapshot_a_report.compare(snapshot_b_report)\ncomparison_report.to_file(\"comparison_report.html\")\n</code></pre> <p>Now you're all set to start exploring your data streams! Bytewax takes care of all the processes necessary to handle and structure data streams into snapshots, which can then be summarized and compared with ydata-profiling through a comprehensive report of data characteristics.</p> <p></p>"},{"location":"integrations/great_expectations/","title":"Great Expectations","text":"<p>Package versions</p> <p>Great expectations integration is no longer supported.  You can recreate the integration with the following packages versions: </p> <pre><code>- ydata-profiling==2.1.0 \n- great-expectations==0.13.4\n</code></pre> <p>Great Expectations is a Python-based open-source library for validating, documenting, and profiling your data. It helps you to maintain data quality and improve communication about data between teams. With Great Expectations, you can assert what you expect from the data you load and transform, and catch data issues quickly -- Expectations are basically unit tests for your data. <code>ydata-profiling</code> features a method to create a suite of Expectations based on the results of your <code>ProfileReport</code>!</p>"},{"location":"integrations/great_expectations/#about-great-expectations","title":"About Great Expectations","text":"<p>Expectations are assertions about your data. In Great Expectations, those assertions are expressed in a declarative language in the form of simple, human-readable Python methods. For example, in order to assert that you want values in a column <code>passenger_count</code> in your dataset to be integers between 1 and 6, you can say:</p> <p><code>expect_column_values_to_be_between(column=\"passenger_count\", min_value=1, max_value=6)</code></p> <p>Great Expectations then uses this statement to validate whether the column <code>passenger_count</code> in a given table is indeed between 1 and 6, and returns a success or failure result. The library currently provides several dozen highly expressive built-in Expectations, and allows you to write custom Expectations.</p> <p>Great Expectations renders Expectations to clean, human-readable documentation called Data Docs. These HTML docs contain both your Expectation Suites as well as your data validation results each time validation is run -- think of it as a continuously updated data quality report.</p> <p>For more information about Great Expectations, check out the Great Expectations documentation and join the Great Expectations Slack channel for help.</p>"},{"location":"integrations/great_expectations/#creating-expectation-suites-with-ydata-profiling","title":"Creating Expectation Suites with ydata-profiling","text":"<p>An Expectation Suite is simply a set of Expectations. You can create Expectation Suites by writing out individual statements, such as the one above, or by automatically generating them based on profiler results.</p> <p><code>ydata-profiling</code> provides a simple <code>to_expectation_suite()</code> method that returns a Great Expectations <code>ExpectationSuite</code> object which contains a set of Expectations.</p> <p>Pre-requisites: In order to run the <code>to_expectation_suite()</code> method, you will need to install Great Expectations with <code>pip install great_expectations</code></p> <p>If you would like to use the additional features such as saving the Suite and building Data Docs, you will also need to configure a Great Expectations Data Context by running <code>great_expectations init</code> in your project\\'s directory.</p> Get your set of expectations<pre><code>import pandas as pd\nfrom ydata_profiling import ProfileReport\n\ndf = pd.read_csv(\"titanic.csv\")\n\nprofile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n\n# Obtain an Expectation Suite with a set of default Expectations\n# By default, this also profiles the dataset, saves the suite, runs validation, and builds Data Docs\nsuite = profile.to_expectation_suite()\n</code></pre> <p>This assumes that the <code>great_expectations</code> Data Context directory is in the same path where you run the script. In order to specify the location of your Data Context, pass it in as an argument:</p> Generate a suite of expectations<pre><code>import great_expectations as ge\n\ndata_context = ge.data_context.DataContext(\n    context_root_dir=\"/Users/panda/code/my_ge_project/\"\n)\nsuite = profile.to_expectation_suite(data_context=data_context)\n</code></pre> <p>You can also configure each feature individually in the function call:</p> Configure features<pre><code>suite = profile.to_expectation_suite(\n    suite_name=\"titanic_expectations\",\n    data_context=data_context,\n    save_suite=False,\n    run_validation=False,\n    build_data_docs=False,\n    handler=handler,\n)\n</code></pre> <p>See the Great Expectations Examples for complete examples.</p> <p></p>"},{"location":"integrations/ides/","title":"IDEs","text":"<p>The package can be directly consumed in some Integrated Development Environments, such as PyCharm.</p>"},{"location":"integrations/ides/#pycharm","title":"PyCharm","text":"<ol> <li>Install <code>ydata-profiling</code> via     <code>../getting_started/installation</code></li> <li>Locate your <code>ydata-profiling</code> executable.</li> </ol> <p>On macOS / Linux / BSD:</p> <pre><code>$ which ydata_profiling\n(example) /usr/local/bin/ydata_profiling\n</code></pre> <p>On Windows:</p> <pre><code>$ where ydata_profiling\n(example) C:\\ProgramData\\Anaconda3\\Scripts\\ydata_profiling.exe\n</code></pre> <ol> <li>In PyCharm, go to Settings (or Preferences on macOS)  Tools External tools</li> <li>Click the + icon to add a new external tool</li> <li> <p>Insert the following values</p> </li> <li> <p>Name: <code>Data Profiling</code></p> <ul> <li>Program: The location obtained in step 2</li> <li>Arguments:      <code>\"$FilePath$\" \"$FileDir$/$FileNameWithoutAllExtensions$_report.html\"</code></li> <li>Working Directory: <code>$ProjectFileDir$</code></li> </ul> </li> </ol> <p>{.align-center width=\"400px\"}</p> <p>To use the PyCharm Integration, right click on any dataset file and External Tools Data Profiling.</p> <p></p>"},{"location":"integrations/interactive_applications/","title":"Interactive applications","text":"<p>The <code>ydata-profiling</code> report, through several of its interfaces, can be integrated in interactive data applications such as those developed with Streamlit or Panel.</p>"},{"location":"integrations/interactive_applications/#streamlit","title":"Streamlit","text":"<p>Streamlit is an open-source Python library made to build web-apps for machine learning and data science.</p> <p>Note</p> <p>This feature is only available for versions previous to ydata-profiling (&lt;=3.6.2).</p> <p></p> Creating a simple Streamlit app with ydata-profiling<pre><code>import pandas as pd\nimport ydata_profiling\nimport streamlit as st\nfrom streamlit_pandas_profiling import st_profile_report\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n)\npr = df.profile_report()\n\nst.title(\"Profiling in Streamlit\")\nst.write(df)\nst_profile_report(pr)\n</code></pre> <p>You can install the ydata-profiling component for Streamlit with pip.</p> <pre><code>pip install streamlit-pandas-profiling\n</code></pre>"},{"location":"integrations/interactive_applications/#dash","title":"Dash","text":"<p>Dash is a Python framework for building machine learning &amp; data science web apps, built on top of Plotly.js, React and Flask. It is commonly used for interactive data exploration, precisely where <code>ydata-profiling</code> also focuses. Inline access to the insights provided by <code>ydata-profiling</code> can help guide the exploratory work allowed by Dash. To integrate a Profiling Report inside a Dash app, two options exist:</p>"},{"location":"integrations/interactive_applications/#load-html-version-of-report-as-an-asset","title":"Load HTML version of report as an asset","text":"<p>Assuming the HTML version of the report is in <code>report.html</code>, move it to a folder called <code>assets</code>. The snippet below shows a simple Dash app, <code>app.py</code>, embedding this report:</p> Create a Dash dashboard with ydata-profiling integrated<pre><code>import dash\nfrom dash import html\n\napp = dash.Dash(__name__)\n\napp.layout = html.Div(\n    children=[\n        html.Iframe(\n            src=\"assets/census_report.html\",  # must be under assets/ to be properly served\n            style={\"height\": \"1080px\", \"width\": \"100%\"},\n        )\n    ]\n)\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True)\n</code></pre> <p>When running <code>python app.py</code>, a Dash app with the report embedded will be available on http://127.0.0.1:8050.</p>"},{"location":"integrations/interactive_applications/#directly-embed-the-raw-html","title":"Directly embed the raw HTML","text":"<p>A more unorthodox option requiring no explicit file handling involves using the <code>dash-dangerously-set-inner-html</code> library to directly embed the HTML raw text (thus requiring no HTML export). Install the library through <code>pip</code>:</p> <pre><code>pip install dash-dangerously-set-inner-html\n</code></pre> <p>And configure the Dash app as in the following snippet:</p> Embed the raw html into Dash<pre><code>import pandas as pd\nfrom ydata_profiling import ProfileReport\nimport dash\nfrom dash import html\nimport dash_dangerously_set_inner_html\n\n# Creating the Report\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n)\nprofile = ProfileReport(df, title=\"Titanic Dataset\")\ntext_raw = profile.to_html()\n\n# Creating the Dash app\n\napp = dash.Dash(__name__)\n\napp.layout = html.Div(\n    [dash_dangerously_set_inner_html.DangerouslySetInnerHTML(text_raw)]\n)\n\napp.layout = html.Div(\n    [dash_dangerously_set_inner_html.DangerouslySetInnerHTML(text_raw)]\n)\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True)\n</code></pre> <p>When running <code>python app.py</code>, a Dash app with the report embedded will be available on <code>&lt;http://127.0.0.1:8050&gt;`_. While this option is somewhat more direct, **the embedded report will not be fully interactive, with some buttons unclickable**.    Panel -----  For more information on how to use</code>ydata-profiling`[ in Panel, see `this GitHub issue \\&lt;https://github.com/ydataai/ydata-profiling/issues/491&gt;]{.title-ref}_ and this integration example.</p> <p></p>"},{"location":"integrations/other_dataframe_libraries/","title":"Other DataFrame libraries","text":"<p><code>ydata-profiling</code> is built on <code>pandas</code> and <code>numpy</code>. Pandas supports a wide range of data formats including CSV, XLSX, SQL, JSON, HDF5, SAS, BigQuery and Stata. Read more on supported formats by Pandas.</p> <p>If you have data in another framework of the Python Data ecosystem, you can use <code>ydata-profiling</code> by converting to a pandas <code>DataFrame</code>, as direct integrations are not yet supported. Large datasets might require sampling (as seen in our documentation on how to profile large datasets).</p> Dask to Pandas<pre><code># Convert dask DataFrame to a pandas DataFrame\ndf = df.compute()\n</code></pre> Vaex to Pandas<pre><code># Convert vaex DataFrame to a pandas DataFrame\ndf = df.to_pandas_df()\n</code></pre> <p>Modin interface</p> <p>This is not part of the API as pandas.DataFrame, naturally, does not posses such a method. You can use the private method DataFrame._to_pandas() to do this conversion. If you would like to do this through the official API you can always save the Modin DataFrame to storage (csv, hdf, sql, ect) and then read it back using Pandas. This will probably be the safer way when working big DataFrames, to avoid out of memory issues.\" Source: https://github.com/modin-project/modin/issues/896</p> Modin to Pandas<pre><code># Convert modin DataFrame to pandas DataFrame\ndf = df._to_pandas()\n</code></pre> <p></p>"},{"location":"integrations/pipelines/","title":"Pipelines","text":"<p>With Python, command-line and Jupyter interfaces, <code>ydata-profiling</code> integrates seamlessly with DAG execution tools like Airflow, Dagster, Kedro and Prefect, allowing it to easily becomes a building block of data ingestion and analysis pipelines. Integration with Dagster or Prefect can be achieved in a similar way as with Airflow.</p>"},{"location":"integrations/pipelines/#ydata-fabric-pipelines","title":"YData Fabric pipelines","text":"<p>Fabric Community version</p> <p>YData Fabric has a community version that you can start using today to create data workflows with pipelines.  Sign up here and start building your pipelines. ydata-profiling is installed by default in all YData images.</p> <p></p> <p>YData Fabric's data pipelines are engineered to harness the capabilities of Kubeflow, providing a robust foundation for scalable and efficient data workflows.  This technical integration ensures that data pipelines can seamlessly handle high data volumes and execute operations with optimal resource utilization.</p> <p>YData Fabric simplifies the process of data pipeline setup by abstracting complexity.  The setup is done through a drag-and-drop experience while leveraging existing Jupyter Notebook environments.  Check this video to see how to create a pipeline in YData Fabric.</p> Profile a csv with ydata-profiling in a pipeline<pre><code># Import required packages\nimport json\n\nimport pandas as pd\nfrom ydata.profiling import ProfileReport\n\n# Read your dataset as a CSV\ndataset = pd.read_csv('data.csv')\n\n# Instantiate the report\nreport = ProfileReport(dataset, title=\"Profiling my data\")\nreport.config.html.navbar_show = False #disable the navigation bar\n\n\n# get the report html\nreport_html = report.to_html()\n\n#Output visually in the pipeline\nmetadata = {\n    'outputs' : [\n        {\n      'type': 'web-app',\n      'storage': 'inline',\n      'source': report_html,\n    }\n    ]\n  }\n\n#write the visual outputs in the pipeline flow\nwith open('mlpipeline-ui-metadata.json', 'w') as metadata_file:\n    json.dump(metadata, metadata_file)\n</code></pre> <p>You can find the notebook with this implementation in ydata-profiling examples folder. </p>"},{"location":"integrations/pipelines/#airflow","title":"Airflow","text":"<p>Integration with Airflow can be easily achieved through the BashOperator or the PythonOperator.</p> ydata-profiling with Airflow<pre><code># Using the command line interface\nprofiling_task = BashOperator(\n    task_id=\"Profile Data\",\n    bash_command=\"pandas_profiling dataset.csv report.html\",\n    dag=dag,\n)\n</code></pre> ydata-profiling with Airflow<pre><code># Using the Python interface\nimport ydata_profiling\n\ndef profile_data(file_name, report_file):\n    df = pd.read_csv(file_name)\n    report = pandas_profiling.ProfileReport(df, title=\"Profiling Report in Airflow\")\n    report.to_file(report_file)\n\n    return \"Report generated at {}\".format(report_file)\n\n\nprofiling_task2 = PythonOperator(\n    task_id=\"Profile Data\",\n    op_kwargs={\"file_name\": \"dataset.csv\", \"report_file\": \"report.html\"},\n    python_callable=profile_data,\n    dag=dag,\n)\n</code></pre>"},{"location":"integrations/pipelines/#kedro","title":"Kedro","text":"<p>There is a community created Kedro plugin available.</p> <p></p>"},{"location":"integrations/pyspark/","title":"\u26a1 Pyspark","text":"<p>\"Spark support</p> <p>Spark dataframes support - Spark Dataframes profiling is available from ydata-profiling version 4.0.0 onwards</p> <p>Data Profiling is a core step in the process of developing AI solutions. For small datasets, the data can be loaded into memory and easily accessed with Python and pandas dataframes. However, for larger datasets what can be done?</p> <p>Big data engines, that distribute the workload through different machines, are the answer. Particularly, Spark rose as one of the most used and adopted engines by the data community. <code>ydata-profiling</code> provides an ease-to-use interface to generate complete and comprehensive data profiling out of your Spark dataframes with a single line of code.</p> <p>Getting started </p>"},{"location":"integrations/pyspark/#installing-pyspark-for-linux-and-windows","title":"Installing Pyspark for Linux and Windows","text":"<p>Tip</p> Ensure that you first install the system requirements (spark and java). <ul> <li>Go to Download Java     JDK     and download the Java Development Kit (JDK).</li> <li>Download and install a Spark version bigger than     3.3</li> <li>Set your environment variables</li> </ul> <pre><code>export SPARK_VERSION=3.3.0\nexport SPARK_DIRECTORY=/opt/spark\nexport HADOOP_VERSION=2.7\nmkdir -p ${SPARK_DIRECTORY}\nsudo apt-get update\nsudo apt-get -y install openjdk-8-jdk\ncurl https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n--output ${SPARK_DIRECTORY}/spark.tgz\ncd ${SPARK_DIRECTORY} &amp;&amp; tar -xvzf spark.tgz &amp;&amp; mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} sparkenv\n</code></pre> <p>A more detailed tutorial for the installation can be found here.</p>"},{"location":"integrations/pyspark/#installing-pyspark-for-macos","title":"Installing Pyspark for MacOS","text":"<p>Use <code>Homebrew</code> to ensure that the system requirements are installed (java and scala (optional)) </p> <pre><code>console brew\ninstall &lt;openjdk@11&gt;\n</code></pre> <pre><code>#Install scala is optional\nbrew install scala\n</code></pre>"},{"location":"integrations/pyspark/#install-pyspark","title":"Install pyspark","text":"<pre><code>brew install apache-spark\n</code></pre> <p>After successful installation of Apache Spark run pyspark from the command line to launch PySpark shell and confirm both python and pyspark versions. A more detailed tutorial for the installation can be found here</p>"},{"location":"integrations/pyspark/#install-ydata-profiling","title":"Install ydata-profiling","text":"<p>Create a pip virtual environment or a conda environment and install <code>ydata-profiling</code> with pyspark as a dependency</p> <pre><code>pip install ydata-profiling[pyspark]\n</code></pre>"},{"location":"integrations/pyspark/#profiling-with-spark-supported-features","title":"Profiling with Spark - Supported Features","text":"<p>Minimal mode</p> <p>This mode was introduced in version v4.0.0</p> <p><code>ydata-profiling</code> now supports Spark Dataframes profiling. You can find an example of the integration here.</p> Features supported: <ul> <li>Univariate variables' analysis</li> <li>Head and Tail dataset sample</li> <li>Correlation matrices: Pearson and Spearman</li> </ul> Coming soon <ul> <li>Missing values analysis</li> <li>Interactions</li> <li>Improved histogram computation</li> </ul>"},{"location":"integrations/pyspark/#profiling-with-spark-dataframes","title":"Profiling with Spark DataFrames","text":"<p>A quickstart example to profile data from a CSV leveraging Pyspark engine and <code>ydata-profiling</code>.</p> Profiling with Spark Dataframes<pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder().master(\"local[1]\")\n      .appName(\"SparkByExamples.com\")\n      .getOrCreate()\n\ndf = spark.read.csv(\"{insert-file-path}\")\n\ndf.printSchema()\n\na = ProfileReport(df)\na.to_file(\"spark_profile.html\")\n</code></pre>"},{"location":"integrations/pyspark/#ydata-profiling-in-databricks","title":"ydata-profiling in Databricks","text":"<p>Yes! We have fantastic new coming with a full tutorial on how you can use ydata-profiling in Databricks Notebooks.</p> <p>The notebook example can be found here.</p> <p>Stay tuned - we are going to update the documentation soon!</p> <p></p>"},{"location":"reference/history/","title":"History &amp; community","text":"<p>The <code>ydata-profiling</code> project became what it is today due to the work of the creators to make it successful. This page aims to highlight a bit of the development history. For the full picture, have a look at the contributor history.</p> <p>YData is the company behind this successful package being responsible  for releases such as the support for time-series, compare datasets and spark support. </p>"},{"location":"reference/history/#thank-you-to-our-amazing-contributors","title":"Thank you to our amazing contributors","text":"<p>A big thank you to all our amazing contributors! </p> <p> </p> <p>Contributors wall made with contrib.rocks.</p>"},{"location":"reference/history/#inception","title":"Inception","text":"<p>In 2016, Jos Polfliet was working for SAS Institute and was getting bored with doing the same types of exploratory data analysis over and over again. Automating his own logic, he noticed it was useful and decided to open-source it under the MIT License. The package was named <code>pandas-profiling</code> as a contraction of pandas and data profiling. The idea was to enable the user to perform automated exploratory data analysis, beyond what the <code>df.describe()</code> function was offering and by abusing Jupyter\\'s HTML output. Since that start, human years of repetitive plotting and summary statistics have been saved from the Machine Learning community.</p>"},{"location":"reference/history/#second-life","title":"Second life","text":"<p>Since May 2019, principal development has been taken over by Simon Brugman. The startup that he co-founded was an early adopter of the package, and he heavily invested in growing the package with experience brought from using it in the industry. Simon led the package through a huge refactor (99.5% was changed) and two major releases, and great collaborations, most notably with Ian Eaves in visions.</p> <p>Profiling as part of Data-Centric AI -----------Since February 2022, YData as committed to continuous support and improvement of <code>pandas-profiling</code>. Our drive as maintainers of the package is to make data scientist fall in love by the ease and quality of profiling delivered by one of the best profiling packages. Since 2022, YData team have already delivered several new releases, that included major features such as Time-Series datasets analysis, compare of 2 datasets and most recently integration with big data engine, Spark.</p> <p><code>pandas-profiling</code> has been named one of the Top 20 ML packages by Google.</p> <p>A huge thank you to 2 of the most iconic contributors who made possible to compare 2 dataset Simon Brugman, and to take the scale of profiling to another level with Spark Edwin Chan.</p>"},{"location":"reference/history/#where-are-we-now","title":"Where are we now?","text":"<p>At the time of writing, <code>pandas-profiling</code> is receiving a new face and name <code>ydata-profiling</code>. Derived from the most recent and major feature, Spark support, we have decided to move from [pandas]{.title-ref} to a name that opens the possibility of new integrations and developments.</p> <p>This is the most popular tool in the world for data exploration in Python, counting with &gt; 11k Github stars, 50 million downloads and users working in any industry, including many at FAANG, banks and insurance companies, startups and universities.</p>"},{"location":"reference/history/#whats-next","title":"What's next?","text":"<p><code>ydata-profiling</code> is committed to the mission of helping data-scientists to adopt a Data-Centric approach towards the development of AI. Continuous development and support will to be part of the development of one of the most beloved open-sources by the data science community.</p> <p>New features are expected, and it will be important to learn from you your needs and expectations so the future can be even brighter. Join the  DCAI community and let us know your thoughts.</p> <p></p>"},{"location":"reference/resources/","title":"Resources","text":"<p>This page contains a series of community-contributed resources that can be found throughout the web. Found, written or recorded a contribution? Feel free to contribute it via a pull request on GitHub.</p>"},{"location":"reference/resources/#notebooks","title":"Notebooks","text":"<ul> <li>Google Cloud Platform: Building a propensity model for financial     services on Google     Cloud</li> <li>Kaggle: Notebooks using     ydata-profiling (previously cally pandas-profiling)     (100+ notebooks)</li> <li>Pycaret: Intermediate Level Tutorials include     pandas-profiling</li> <li>Google BigQuery integration Notebook: Building a propensity model for financial services on Google Cloud</li> </ul>"},{"location":"reference/resources/#articles","title":"Articles","text":"<ul> <li>Pandas-profiling now supports spark   (Fabiana, Miriam and Corey, Apr 3, 2023)</li> <li>How to an EDA on Time-series data   (Fabiana Clemente, Oct 21, 2022)</li> <li>Evaluation of freely available data profiling tools for health data     research application: a functional evaluation     review (Ben     Gordon, May 9, 2022)</li> <li>Generate Reports Using Pandas Profiling, Deploy Using     Streamlit     (Kaustubh Gupta, June 25, 2021)</li> <li>Bringing Customization to Pandas     Profiling     (Ian Eaves, March 5, 2021)</li> <li>Beginner Friendly Data Science Projects Accepting     Contributions     (Adam Ross Nelson, January 18, 2021)</li> <li>Pandas profiling and exploratory data analysis with line one of     code!     (Magdalena Konkiewicz, Jun 10, 2020)</li> <li>The Covid 19 health     issue     (Concillier Kitungulu, April 20, 2020)</li> <li>Accelerate Your Exploratory Data Analysis With     Pandas-Profiling     (Sukanta Roy, April 19, 2020)</li> <li>Pandas-Profiling, explore your data faster in     Python (Gabriel Fuhr, May     31, 2020)</li> <li>Pandas One Line Magical Code for EDA: Pandas Profile     Report     (Sunil Kappal, July 8, 2019)</li> <li>Speed Up Your Exploratory Data Analysis With     Pandas-Profiling     (Lukas Frei, April 26, 2019)</li> <li>Pandas Exploratory Data Analysis: Data Profiling with one single     command     (Vinay Babu, January 15, 2019)</li> <li>Data Profiling with     pandas-profiling     (Lee Honan, October 28, 2018)</li> <li>3 Must-have tools if you\\'re serious about machine     learning     (Willem Meints, August 21, 2018)</li> <li>Exploratory Data Analysis of Craft Beers: Data     Profiling     (Jean-Nicholas Hould, April 13, 2017)</li> </ul>"},{"location":"reference/resources/#videos","title":"Videos","text":"<ul> <li>How to install ydata-profiling with conda and Python 3.11   (Fabiana Clemente, June 2023, 2023)</li> <li>Installing Anaconda, Creating a virtual environment and installing     pandas-profiling in it     (Abhiram R - EverythingPython, Jan 25, 2022)</li> <li>How to Install and Use Pandas Profiling on Google     Colab (Chanin     Nantasenamat, Apr 25, 2020)</li> <li>Data Exploration in Python with Just one line of code | Python     Tutorial (Abhishek     Agarrwal, Nov 7, 2019)</li> <li>Pandas Profiling Correlations | pythonbeginner     2019 (August 5, 2019)</li> <li>Automated Data Profiling using Python Pandas     (pandas-profiling)     (Kunaal Naik, Oct 14, 2019)</li> <li>Data Science Tools - Data Profiling with Pandas (Simple Exploratory     Data Analysis)     (Jesse E. Agbe, Jun 20, 2019)</li> <li>Exploratory data analysis in python - PyCon     2017 (Chloe Mawer,     Jonathan Whitmore, May 18, 2017)</li> </ul>"},{"location":"reference/resources/#books","title":"Books","text":"<ul> <li>Machine Learning Pocket     Reference     by Matt Harrison, August 2019, ISBN: 9781492047544</li> </ul> <ul> <li>Python Data Science     Essentials     by Alberto Boschetti and Luca Massaron, September 2018, ISBN:     9781789537864</li> </ul>"},{"location":"reference/resources/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Penn Machine Learning Benchmarks     (PMLB)     (description)</li> <li>dabl package</li> </ul>"},{"location":"support-contribution/common_issues/","title":"Common issues","text":""},{"location":"support-contribution/common_issues/#typeerror-_plot_histogram-got-an-unexpected-keyword-argument-title","title":"TypeError: _plot_histogram() got an unexpected keyword argument \\'title\\'","text":"<p>This error occurs when using outdated versions of the package.</p> <p>Ensure that you are using the latest version, and when in a notebook, ensure that you\\'ve restarted the kernel when needed. Also make sure that you install in the right Python environment (please use <code>!{sys.executable} -m pip install -U ydata-profiling</code>!). More information on installing Python packages directly from a notebook: \\'Installing Python Packages from a Jupyter Notebook\\'.</p> <p>Related GitHub issues:</p> <ul> <li>[950]</li> <li>[939]</li> <li>[528]</li> <li>[485]</li> <li>[396]</li> </ul>"},{"location":"support-contribution/common_issues/#jupyter-intslidervalue0","title":"Jupyter \\\"IntSlider(value=0)\\\"","text":"<p>When in a Jupyter environment, if only text such as <code>IntSlider(value=0)</code> or <code>(children=(IntSlider(value=0, description='x', max=1), Output()), _dom_classes=('widget-interact',))</code> is shown, then the Jupyter Widgets are not activated. The <code>../getting_started/installation</code> page contains instructions on how to resolve this problem.</p>"},{"location":"support-contribution/common_issues/#memoryerror-unable-to-allocate-when-profiling-datasets-with-very-large-values","title":"MemoryError: Unable to allocate... when profiling datasets with very large values","text":"<p>A memory error that comes up when profiling datasets with large outliers (even if the dataset itself is small), which is due to an underlying bug in <code>numpy</code>, used to build a histogram. Although some workarounds are suggested on numpy\\'s GitHub, the bug is not yet fixed. One workaround is to filter out large outliers prior to report computation.</p> <p>Related StackOverflow questions: -   MemoryError when using ydata_profiling     profile_report</p> <p></p>"},{"location":"support-contribution/contribution_guidelines/","title":"Contribution Guidelines","text":""},{"location":"support-contribution/contribution_guidelines/#contributing-a-new-feature","title":"Contributing a new feature","text":"<ul> <li>Open a new GitHub pull request with the patch.</li> <li>Ensure the PR description clearly describes the problem and     solution. Include the relevant issue number if applicable.</li> </ul>"},{"location":"support-contribution/contribution_guidelines/#development-tooling","title":"Development tooling","text":"<p>To ease the development cycle, some tools are available. These can be called from the root directory with the <code>make</code> command.</p> <p>The following commands are supported:</p> <pre><code>make lint\nmake install\nmake examples\nmake docs\nmake test\nmake clean\n</code></pre>"},{"location":"support-contribution/contribution_guidelines/#contribution-quality-standards","title":"Contribution quality standards","text":"<p>To guarantee a high quality of the contributed code, the project workflow validates the added modification as well as the introduced commit messages. The same mechanisms are used locally to find and solve existing issues before submitting a pull request.</p> <p>To activate the local mechanisms (created using pre-commit hooks), run the following commands:</p> <pre><code>pip install -r requirements-dev.txt\npre-commit install --hook-type commit-msg --hook-type pre-commit\n</code></pre>"},{"location":"support-contribution/contribution_guidelines/#git-workflow","title":"Git workflow","text":"<p>The Git workflow used in this project is based on this blog post. Using this workflow allows for better collaboration between contributors and automation of repetitive tasks.</p> <p>In addition to the workflow described in the blog post, Github Actions lints the code automatically on the release branches and builds documentation from each push to the master branch. For now, we don\\'t use hotfix branches.</p> <p>Branch naming:</p> <ul> <li>develop: development branch </li> <li>master: master branch </li> <li>feat/[FEATURE NAME]: feature branches </li> <li>docs/[branch name]: documentation and examples</li> </ul> <p></p>"},{"location":"support-contribution/contribution_guidelines/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>This package does not have a Contributor License Agreement (CLA), as the GitHub Terms of Service provides a sensible explicit default:</p> <p>Whenever you make a contribution to a repository containing notice of a license, you license your contribution under the same terms, and you agree that you have the right to license your contribution under those terms.</p> <p>Read Github\\'s open source legal guide for further details.</p>"},{"location":"support-contribution/contribution_guidelines/#discord-data-centric-community","title":"Discord Data-centric community","text":"<p>The community is low-threshold place to ask questions regarding development and contributing. Join the Discord community.</p>"},{"location":"support-contribution/contribution_guidelines/#more-information","title":"More information","text":"<p>Read more on getting involved in the Contribution Guide available on GitHub.</p> <p></p>"},{"location":"support-contribution/help_troubleshoot/","title":"Help &amp; Troubleshooting","text":""},{"location":"support-contribution/help_troubleshoot/#troubleshooting","title":"Troubleshooting","text":"<p>To start troubleshooting, we need to trace the issue to a bug in the code or to something else (such as your local environment). The first step is to create a new environment with a fresh installation (see Installation guide for instructions). In many cases, the problem will be resolved by this step.</p> <p>If the problem can be replicated in the new environment, then it likely is a software bug. Before proceeding, check <code>common_issues</code> to check whether it is a previously identified common issue.</p>"},{"location":"support-contribution/help_troubleshoot/#reporting-a-bug","title":"Reporting a bug","text":"<p>To ensure the bug was not already reported by searching on Github under Issues. If you\\'re unable to find an open issue addressing the problem, open a new one. If possible, use the relevant bug report templates to create the issue.</p> <p>You should provide the minimal information to reproduce this bug. This guide can help in crafting a minimal bug report. Please include:</p> <ul> <li> <p>The minimal code you are using to generate the report</p> </li> <li> <p>Version information is essential in reproducing and resolving bugs.     Include relevant environment details such as:</p> <ul> <li>operating system (e.g. Windows, Linux, Mac)</li> <li>Python version (e.g. <code>3.7</code>)</li> <li>Interface: Jupyter notebook (or cloud services like Google     Colab, Kaggle Kernels, etc), console or IDE (such as     PyCharm,VS Code,etc)</li> <li>package manager (e.g. <code>pip --version</code> or <code>conda info</code>)</li> <li>packages (<code>pip freeze &gt; packages.txt</code> or <code>conda list</code>). Please     make sure this is contained in a collapsed section     (instructions below)</li> </ul> </li> <li> <p>a sample of the dataset (<code>df.sample()</code> or <code>df.head()</code>). If the     dataset is confidential, for example when it contains     company-sensitive information, provide us with a synthetic or open     dataset that produces the same error. You can anonymize the column     names if necessary.</p> </li> <li> <p>a description of the dataset and its structure, for example by     reporting the DataFrame\\'s structure through the output of     <code>df.info()</code>.</p> </li> </ul>"},{"location":"support-contribution/help_troubleshoot/#issue-formatting","title":"Issue formatting","text":"<p>To craft helpful and easily readable issues, two formatting tricks are recommended:</p> <ul> <li>Code highlighting: wrap all code and error messages in fenced     blocks, and in particular add the language identifier. Check the     Github docs on highlighting code     blocks     for details.</li> <li>Collapsed sections: organize long error messages and requirement     listings in collapsed sections. The Github docs on collapsed     sections     provide detailed information.</li> </ul>"},{"location":"support-contribution/help_troubleshoot/#using-stack-overflow","title":"Using Stack Overflow","text":"<p>Users with a request for help on how to use <code>ydata-profiling</code> should consider asking their question on Stack Overflow, under the dedicated <code>ydata-profiling</code> tag:</p> <p> or,  </p> <p>for questions about <code>ydata-profiling</code> older versions.</p>"},{"location":"support-contribution/help_troubleshoot/#discord-community","title":"Discord community","text":"<p>Join the Discord community to connect with both other users and developers that might be able to answer your questions. The #ydata-profiling and #need-help channels are recommended for questions and issues.</p> <p></p>"}]}